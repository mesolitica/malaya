{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "This tutorial is available as an IPython notebook at [Malaya/example/extractive-summarization](https://github.com/huseinzol05/Malaya/tree/master/example/extractive-summarization).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.74 s, sys: 636 ms, total: 5.37 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import malaya\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available skip-thought models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>55.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual-network</th>\n",
       "      <td>99.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Size (MB)\n",
       "lstm                   55.4\n",
       "residual-network       99.7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.summarization.extractive.available_skipthought()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ``'lstm'`` - LSTM skip-thought deep learning model trained on news dataset.\n",
    "* ``'residual-network'`` - CNN residual network with Bahdanau Attention skip-thought deep learning model trained on wikipedia dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "We use TextRank from networkx for scoring algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isu_kerajaan = [\n",
    "    'Kenyataan kontroversi Setiausaha Agung Barisan Nasional (BN), Datuk Seri Mohamed Nazri Aziz berhubung sekolah vernakular merupakan pandangan peribadi beliau',\n",
    "    'Timbalan Presiden UMNO, Datuk Seri Mohamad Hasan berkata, kenyataan tersebut tidak mewakili pendirian serta pandangan UMNO \\n\\nkerana parti itu menghormati serta memahami keperluan sekolah vernakular dalam negara',\n",
    "    '\"Saya ingin menegaskan dua perkara penting',\n",
    "    'Pertama pendirian beliau tersebut adalah pandangan peribadi yang tidak mewakili pendirian dan pandangan UMNO',\n",
    "    '\"Kedua UMNO sebagai sebuah parti sangat menghormati dan memahami keperluan sekolah vernakular di Malaysia',\n",
    "    'UMNO berpendirian sekolah jenis ini perlu terus wujud di negara kita,\" katanya dalam satu kenyataan akhbar malam ini',\n",
    "    'Mohamed Nazri semalam menjelaskan, kenyataannya mengenai sekolah jenis kebangsaan Cina dan Tamil baru-baru ini disalah petik pihak media',\n",
    "    'Kata Nazri dalam kenyataannya itu, beliau menekankan bahawa semua pihak perlu menghormati hak orang Melayu dan bumiputera',\n",
    "    'Mohamad yang menjalankan tugas-tugas Presiden UMNO berkata, UMNO konsisten dengan pendirian itu dalam mengiktiraf kepelbagaian bangsa dan etnik termasuk hak untuk beragama serta mendapat pendidikan',\n",
    "    'Menurut beliau, persefahaman dan keupayaan meraikan kepelbagaian itu menjadi kelebihan dan kekuatan UMNO dan BN selama ini',\n",
    "    'Kata beliau, komitmen UMNO dan BN berhubung perkara itu dapat dilihat dengan jelas dalam bentuk sokongan infrastruktur, pengiktirafan dan pemberian peruntukan yang diperlukan',\n",
    "    '\"Saya berharap isu ini tidak dipolitikkan secara tidak bertanggungjawab oleh mana-mana pihak terutama dengan cara yang tidak menggambarkan pendirian sebenar UMNO dan BN,\" katanya',\n",
    "    'Beliau turut menegaskan Mohamed Nazri telah mengambil pertanggungjawaban dengan membuat penjelasan maksud sebenarnya ucapanny di Semenyih, Selangor tersebut',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "isu_string = '\\n\\n\\n\\nDUA legenda hebat dan ‘The living legend’ ini sudah memartabatkan bidang muzik sejak lebih tiga dekad lalu. Jika Datuk Zainal Abidin, 59, dikenali sebagai penyanyi yang memperjuangkan konsep ‘world music’, Datuk Sheila Majid, 55, pula lebih dikenali dengan irama jazz dan R&B.\\n\\nNamun, ada satu persamaan yang mengeratkan hubungan mereka kerana sama-sama mencintai bidang muzik sejak dulu.\\n\\nKetika ditemui dalam sesi fotografi yang diatur di Balai Berita, baru-baru ini, Zainal berkata, dia lebih ‘senior’ daripada Sheila kerana bermula dengan kumpulan Headwind sebelum menempa nama sebagai penyanyi solo.\\n\\n“Saya mula berkawan rapat dengan Sheila ketika sama-sama bernaung di bawah pengurusan Roslan Aziz Productions (RAP) selepas membina karier sebagai artis solo.\\n\\n“Namun, selepas tidak lagi bernaung di bawah RAP, kami juga membawa haluan karier seni masing-masing selepas itu,” katanya.\\n\\nJusteru katanya, dia memang menanti peluang berganding dengan Sheila dalam satu konsert.\\n\\nPenyanyi yang popular dengan lagu Hijau dan Ikhlas Tapi Jauh itu mengakui mereka memang ada keserasian ketika bergandingan kerana membesar pada era muzik yang sama.\\n\\n“Kami memang meminati bidang muzik dan saling memahami antara satu sama lain. Mungkin kerana kami berdua sudah berada pada tahap di puncak karier muzik masing-masing.\\n\\n“Saya bersama Sheila serta Datuk Afdlin Shauki akan terbabit dalam satu segmen yang ditetapkan.\\n\\n“Selain persembahan solo, saya juga berduet dengan Sheila dan Afdlin dalam segmen interaktif ini. Setiap penyanyi akan menyampaikan enam hingga tujuh lagu setiap seorang sepanjang konsert yang berlangsung tiga hari ini,” katanya.\\n\\nBagi Sheila pula, dia memang ada terbabit dengan beberapa persembahan bersama Zainal cuma tiada publisiti ketika itu.\\n\\n“Kami pernah terbabit dengan showcase dan majlis korporat sebelum ini. Selain itu, Zainal juga terbabit dengan Konsert Legenda yang membabitkan jelajah empat lokasi sebelum ini.\\n\\n“Sebab itu, saya sukar menolak untuk bekerjasama dengannya dalam Festival KL Jamm yang dianjurkan buat julung kali dan berkongsi pentas dalam satu konsert bertaraf antarabangsa,” katanya.\\n\\n\\n\\nFESTIVAL KL Jamm bakal menggabungkan pelbagai genre muzik seperti rock, hip hop, jazz dan pop dengan lebih 100 persembahan, 20 ‘showcase’ dan pameran.\\n\\nKonsert berbayar\\n\\n\\n\\nMewakili golongan anak seni, Sheila menaruh harapan semoga Festival KL Jamm akan menjadi platform buat artis yang sudah ada nama dan artis muda untuk membuat persembahan, sekali gus sama-sama memartabatkan industri muzik tempatan.\\n\\nMenurut Sheila, dia juga mencadangkan lebih banyak tempat diwujudkan untuk menggalakkan artis muda membuat persembahan, sekali gus menggilap bakat mereka.\\n\\n“Berbanding pada zaman saya dulu, artis muda sekarang tidak banyak tempat khusus untuk mereka menyanyi dan menonjolkan bakat di tempat awam.\\n\\n“Rata-rata hanya sekadar menyanyi di laman Instagram dan cuma dikenali menerusi satu lagu. Justeru, bagaimana mereka mahu buat showcase kalau hanya dikenali dengan satu lagu?” katanya.\\n\\nPada masa sama, Sheila juga merayu peminat tempatan untuk sama-sama memberi sokongan pada penganjuran festival KL Jamm sekali gus mencapai objektifnya.\\n\\n“Peminat perlu ubah persepsi negatif mereka dengan menganggap persembahan artis tempatan tidak bagus.\\n\\n“Kemasukan artis luar juga perlu dilihat dari sudut yang positif kerana kita perlu belajar bagaimana untuk menjadi bagus seperti mereka,” katanya.\\n\\nSementara itu, Zainal pula berharap festival itu akan mendidik orang ramai untuk menonton konsert berbayar serta memberi sokongan pada artis tempatan.\\n\\n“Ramai yang hanya meminati artis tempatan tetapi tidak mahu mengeluarkan sedikit wang untuk membeli tiket konsert mereka.\\n\\n“Sedangkan artis juga menyanyi untuk kerjaya dan ia juga punca pendapatan bagi menyara hidup,” katanya.\\n\\nFestival KL Jamm bakal menghimpunkan barisan artis tempatan baru dan nama besar dalam konsert iaitu Datuk Ramli Sarip, Datuk Afdlin Shauki, Zamani, Amelina, Radhi OAG, Dr Burn, Santesh, Rabbit Mac, Sheezy, kumpulan Bunkface, Ruffedge, Pot Innuendo, artis dari Kartel (Joe Flizzow, Sona One, Ila Damia, Yung Raja, Faris Jabba dan Abu Bakarxli) dan Malaysia Pasangge (artis India tempatan).\\n\\nManakala, artis antarabangsa pula membabitkan J Arie (Hong Kong), NCT Dream (Korea Selatan) dan DJ Sura (Korea Selatan).\\n\\nKL Jamm dianjurkan Music Unlimited International Sdn Bhd dan bakal menggabungkan pelbagai genre muzik seperti rock, hip hop, jazz dan pop dengan lebih 100 persembahan, 20 ‘showcase’, pameran dan perdagangan berkaitan.\\n\\nFestival tiga hari itu bakal berlangsung di Pusat Pameran dan Perdagangan Antarabangsa Malaysia (MITEC), Kuala Lumpur pada 26 hingga 28 April ini.\\n\\nMaklumat mengenai pembelian tiket dan keterangan lanjut boleh melayari www.kljamm.com.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SKLearn Interface\n",
    "\n",
    "Load decomposition and text vectorizer module from sklearn,\n",
    "\n",
    "```python\n",
    "def sklearn(model, vectorizer):\n",
    "    \"\"\"\n",
    "    sklearn interface for summarization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Should have `fit_transform` method. Commonly:\n",
    "\n",
    "        * ``sklearn.decomposition.TruncatedSVD`` - LSA algorithm.\n",
    "        * ``sklearn.decomposition.LatentDirichletAllocation`` - LDA algorithm.\n",
    "    vectorizer : object\n",
    "        Should have `fit_transform` method. Commonly:\n",
    "\n",
    "        * ``sklearn.feature_extraction.text.TfidfVectorizer`` - TFIDF algorithm.\n",
    "        * ``sklearn.feature_extraction.text.CountVectorizer`` - Bag-of-Word algorithm.\n",
    "        * ``malaya.text.vectorizer.SkipGramCountVectorizer`` - Skip Gram Bag-of-Word algorithm.\n",
    "        * ``malaya.text.vectorizer.SkipGramTfidfVectorizer`` - Skip Gram TFIDF algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: malaya.model.extractive_summarization.SKLearn\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from malaya.text.vectorizer import SkipGramCountVectorizer, SkipGramTfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "stopwords = malaya.text.function.get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = SkipGramCountVectorizer(\n",
    "    max_df = 0.95,\n",
    "    min_df = 1,\n",
    "    ngram_range = (1, 3),\n",
    "    stop_words = stopwords,\n",
    "    skip = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = malaya.summarization.extractive.sklearn(svd, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence level\n",
    "\n",
    "This will predict scores for each sentences,\n",
    "\n",
    "```python\n",
    "def sentence_level(\n",
    "    self,\n",
    "    corpus,\n",
    "    isi_penting: str = None,\n",
    "    top_k: int = 3,\n",
    "    important_words: int = 10,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarize list of strings / string on sentence level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: str / List[str]\n",
    "    isi_penting: str, optional (default=None)\n",
    "        if not None, will put priority based on `isi_penting`.\n",
    "    top_k: int, (default=3)\n",
    "        number of summarized strings.\n",
    "    important_words: int, (default=10)\n",
    "        number of important words.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: {'summary', 'top-words', 'cluster-top-words', 'score'}\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['summary', 'top-words', 'cluster-top-words', 'score'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = model.sentence_level(isu_kerajaan)\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"Kedua UMNO sebagai sebuah parti sangat menghormati dan memahami keperluan '\n",
      " 'sekolah vernakular di Malaysia. kerana parti itu menghormati serta memahami '\n",
      " 'keperluan sekolah vernakular dalam negara. Timbalan Presiden UMNO, Datuk '\n",
      " 'Seri Mohamad Hasan berkata, kenyataan tersebut tidak mewakili pendirian '\n",
      " 'serta pandangan UMNO .')\n"
     ]
    }
   ],
   "source": [
    "pprint(r['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tugas presiden umno',\n",
       " 'nazri',\n",
       " 'vernakular',\n",
       " 'menghormati',\n",
       " 'tugas umno',\n",
       " 'pendirian pandangan',\n",
       " 'sekolah']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['cluster-top-words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kenyataan', 0.07025715941848468),\n",
       " ('kontroversi', 0.07025715941848468),\n",
       " ('Setiausaha', 0.07025715941848468),\n",
       " ('Agung', 0.07025715941848468),\n",
       " ('Barisan', 0.07025715941848468),\n",
       " ('Nasional', 0.07025715941848468),\n",
       " ('(BN),', 0.07025715941848468),\n",
       " ('Datuk', 0.07025715941848468),\n",
       " ('Seri', 0.07025715941848468),\n",
       " ('Mohamed', 0.07025715941848468),\n",
       " ('Nazri', 0.07025715941848468),\n",
       " ('Aziz', 0.07025715941848468),\n",
       " ('berhubung', 0.07025715941848468),\n",
       " ('sekolah', 0.07025715941848468),\n",
       " ('vernakular', 0.07025715941848468),\n",
       " ('merupakan', 0.07025715941848468),\n",
       " ('pandangan', 0.07025715941848468),\n",
       " ('peribadi', 0.07025715941848468),\n",
       " ('beliau.', 0.07025715941848468),\n",
       " ('Timbalan', 0.11720420897327886)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['score'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Beliau turut menegaskan Mohamed Nazri telah mengambil pertanggungjawaban '\n",
      " 'dengan membuat penjelasan maksud sebenarnya ucapanny di Semenyih, Selangor '\n",
      " 'tersebut. Kata Nazri dalam kenyataannya itu, beliau menekankan bahawa semua '\n",
      " 'pihak perlu menghormati hak orang Melayu dan bumiputera. Mohamed Nazri '\n",
      " 'semalam menjelaskan, kenyataannya mengenai sekolah jenis kebangsaan Cina dan '\n",
      " 'Tamil baru-baru ini disalah petik pihak media.')\n"
     ]
    }
   ],
   "source": [
    "r = model.sentence_level(isu_kerajaan, isi_penting = 'Mohamed Nazri')\n",
    "pprint(r['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level\n",
    "\n",
    "This will predict scores for each words. This interface will not returned a summary, just score for each words.\n",
    "\n",
    "```python\n",
    "def word_level(\n",
    "    self,\n",
    "    corpus,\n",
    "    isi_penting: str = None,\n",
    "    window_size: int = 10,\n",
    "    important_words: int = 10,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarize list of strings / string on word level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: str / List[str]\n",
    "    isi_penting: str, optional (default=None)\n",
    "        if not None, will put priority based on `isi_penting`.\n",
    "    window_size: int, (default=10)\n",
    "        window size for each word.\n",
    "    important_words: int, (default=10)\n",
    "        number of important words.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: {'top-words', 'cluster-top-words', 'score'}\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['top-words', 'cluster-top-words', 'score'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = model.word_level(isu_kerajaan, isi_penting = 'Mohamed Nazri')\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kenyataan', 0.16809629126117476),\n",
       " ('kontroversi', 0.2133122956188781),\n",
       " ('Setiausaha', 0.2946152484378591),\n",
       " ('Agung', 0.3335993239450466),\n",
       " ('Barisan', 0.3779873115316621),\n",
       " ('Nasional', 0.4254942849807996),\n",
       " ('(BN),', 0.4402120384348302),\n",
       " ('Datuk', 0.4402120384348302),\n",
       " ('Seri', 0.4398985388456278),\n",
       " ('Mohamed', 0.42780575539932747),\n",
       " ('Nazri', 0.42780575539932747),\n",
       " ('Aziz', 0.41245035042151057),\n",
       " ('berhubung', 0.3879585108632802),\n",
       " ('sekolah', 0.37430313847686075),\n",
       " ('vernakular', 0.3727791362231256),\n",
       " ('merupakan', 0.3683025784505585),\n",
       " ('pandangan', 0.34671436914445564),\n",
       " ('peribadi', 0.32911022738895535),\n",
       " ('beliau.', 0.32911022738895535),\n",
       " ('Timbalan', 0.31232539921078645)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['score'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Doc2Vec interface\n",
    "\n",
    "Doc2Vec interface using WordVector Malaya.\n",
    "\n",
    "```python\n",
    "def doc2vec(wordvector):\n",
    "    \"\"\"\n",
    "    Doc2Vec interface for summarization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    wordvector : object\n",
    "        malaya.wordvector.WordVector object.\n",
    "        should have `get_vector_by_name` method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: malaya.model.extractive_summarization.Doc2Vec\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/wordvector.py:120: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/wordvector.py:131: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_news, embedded_news = malaya.wordvector.load_news()\n",
    "w2v = malaya.wordvector.load(embedded_news, vocab_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = malaya.summarization.extractive.doc2vec(w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence level\n",
    "\n",
    "This will predict scores for each sentences,\n",
    "\n",
    "```python\n",
    "def sentence_level(\n",
    "    self,\n",
    "    corpus,\n",
    "    isi_penting: str = None,\n",
    "    top_k: int = 3,\n",
    "    aggregation = np.mean,\n",
    "    soft: bool = False,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarize list of strings / string on sentence level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: str / List[str]\n",
    "    isi_penting: str, optional (default=None)\n",
    "        if not None, will put priority based on `isi_penting`.\n",
    "    top_k: int, (default=3)\n",
    "        number of summarized strings.\n",
    "    aggregation: Callable, optional (default=numpy.mean)\n",
    "        Aggregation method for Doc2Vec.\n",
    "    soft: bool, optional (default=False)\n",
    "        soft: bool, (default=True)\n",
    "        if True, a word not in the dictionary will be replaced with nearest JaroWinkler ratio.\n",
    "        if False, it will returned embedding full with zeros.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: {'summary', 'score'}\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mohamad yang menjalankan tugas-tugas Presiden UMNO berkata, UMNO konsisten '\n",
      " 'dengan pendirian itu dalam mengiktiraf kepelbagaian bangsa dan etnik '\n",
      " 'termasuk hak untuk beragama serta mendapat pendidikan. Kata Nazri dalam '\n",
      " 'kenyataannya itu, beliau menekankan bahawa semua pihak perlu menghormati hak '\n",
      " 'orang Melayu dan bumiputera. Kata beliau, komitmen UMNO dan BN berhubung '\n",
      " 'perkara itu dapat dilihat dengan jelas dalam bentuk sokongan infrastruktur, '\n",
      " 'pengiktirafan dan pemberian peruntukan yang diperlukan.')\n",
      "CPU times: user 9.18 ms, sys: 1.64 ms, total: 10.8 ms\n",
      "Wall time: 10.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = model.sentence_level(isu_kerajaan)\n",
    "pprint(r['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kenyataan', 0.07027066163158477),\n",
       " ('kontroversi', 0.07027066163158477),\n",
       " ('Setiausaha', 0.07027066163158477),\n",
       " ('Agung', 0.07027066163158477),\n",
       " ('Barisan', 0.07027066163158477),\n",
       " ('Nasional', 0.07027066163158477),\n",
       " ('(BN),', 0.07027066163158477),\n",
       " ('Datuk', 0.07027066163158477),\n",
       " ('Seri', 0.07027066163158477),\n",
       " ('Mohamed', 0.07027066163158477),\n",
       " ('Nazri', 0.07027066163158477),\n",
       " ('Aziz', 0.07027066163158477),\n",
       " ('berhubung', 0.07027066163158477),\n",
       " ('sekolah', 0.07027066163158477),\n",
       " ('vernakular', 0.07027066163158477),\n",
       " ('merupakan', 0.07027066163158477),\n",
       " ('pandangan', 0.07027066163158477),\n",
       " ('peribadi', 0.07027066163158477),\n",
       " ('beliau.', 0.07027066163158477),\n",
       " ('Timbalan', 0.07037457887922696)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['score'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mohamad yang menjalankan tugas-tugas Presiden UMNO berkata, UMNO konsisten '\n",
      " 'dengan pendirian itu dalam mengiktiraf kepelbagaian bangsa dan etnik '\n",
      " 'termasuk hak untuk beragama serta mendapat pendidikan. Kata Nazri dalam '\n",
      " 'kenyataannya itu, beliau menekankan bahawa semua pihak perlu menghormati hak '\n",
      " 'orang Melayu dan bumiputera. Kata beliau, komitmen UMNO dan BN berhubung '\n",
      " 'perkara itu dapat dilihat dengan jelas dalam bentuk sokongan infrastruktur, '\n",
      " 'pengiktirafan dan pemberian peruntukan yang diperlukan.')\n"
     ]
    }
   ],
   "source": [
    "r = model.sentence_level(isu_kerajaan, isi_penting = 'Mohamed Nazri')\n",
    "pprint(r['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level\n",
    "\n",
    "This will predict scores for each words. This interface will not returned a summary, just score for each words.\n",
    "\n",
    "```python\n",
    "def word_level(\n",
    "    self,\n",
    "    corpus,\n",
    "    isi_penting: str = None,\n",
    "    window_size: int = 10,\n",
    "    aggregation = np.mean,\n",
    "    soft: bool = False,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarize list of strings / string on sentence level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: str / List[str]\n",
    "    isi_penting: str, optional (default=None)\n",
    "        if not None, will put priority based on `isi_penting`.\n",
    "    window_size: int, (default=10)\n",
    "        window size for each word.\n",
    "    aggregation: Callable, optional (default=numpy.mean)\n",
    "        Aggregation method for Doc2Vec.\n",
    "    soft: bool, optional (default=False)\n",
    "        soft: bool, (default=True)\n",
    "        if True, a word not in the dictionary will be replaced with nearest JaroWinkler ratio.\n",
    "        if False, it will returned embedding full with zeros.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: {'score'}\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.7 ms, sys: 3.52 ms, total: 75.2 ms\n",
      "Wall time: 72 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('DUA', 0.10217850225566492),\n",
       " ('legenda', 0.10256503728273535),\n",
       " ('hebat', 0.10135587890905765),\n",
       " ('dan', 0.10158261783201081),\n",
       " (\"'The\", 0.10185488894991031),\n",
       " ('living', 0.10207170990828254),\n",
       " (\"legend'\", 0.10205730536399951),\n",
       " ('ini', 0.10202961616004474),\n",
       " ('sudah', 0.10210986074917726),\n",
       " ('memartabatkan', 0.10203142215244121),\n",
       " ('bidang', 0.10433985839774515),\n",
       " ('muzik', 0.10437382317184496),\n",
       " ('sejak', 0.10437382707131633),\n",
       " ('lebih', 0.10444438297719007),\n",
       " ('tiga', 0.1044684279620556),\n",
       " ('dekad', 0.10444221573136775),\n",
       " ('lalu.', 0.10206875552003145),\n",
       " ('Jika', 0.10127214013691332),\n",
       " ('Datuk', 0.10122756975172667),\n",
       " ('Zainal', 0.10127912371730628)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = model.word_level(isu_string, window_size = 5)\n",
    "r['score'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Encoder summarization\n",
    "\n",
    "We leverage the power of deep encoder models like skip-thought or Transformer to do extractive summarization for us.\n",
    "\n",
    "```python\n",
    "def encoder(vectorizer):\n",
    "    \"\"\"\n",
    "    Encoder interface for summarization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vectorizer : object\n",
    "        encoder interface object, eg, BERT, skip-thought, XLNET, ALBERT, ALXLNET.\n",
    "        should have `vectorize` method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: malaya.model.extractive_summarization.Encoder\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/function/__init__.py:74: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/function/__init__.py:76: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:vectorizer model does not have `attention` method, `top-words` will not work\n"
     ]
    }
   ],
   "source": [
    "lstm = malaya.summarization.extractive.deep_skipthought(model = 'lstm')\n",
    "encoder = malaya.summarization.extractive.encoder(lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we loaded an encoder model that do not have `attention` method, it will not returned `top-words`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/xlnet.py:70: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/xlnet.py:70: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/xlnet.py:253: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/xlnet.py:253: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/xlnet.py:253: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/xlnet.py:253: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/modeling.py:697: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/modeling.py:697: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:memory input None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:memory input None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use float type <dtype: 'float32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use float type <dtype: 'float32'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/modeling.py:704: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/modeling.py:704: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/modeling.py:809: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/modeling.py:809: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/modeling.py:109: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/modeling.py:109: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/__init__.py:95: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/__init__.py:95: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/__init__.py:96: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/__init__.py:96: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/__init__.py:100: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/__init__.py:100: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/__init__.py:103: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/Malaya/malaya/transformers/alxlnet/__init__.py:103: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/huseinzolkepli/Malaya/alxlnet-model/base/alxlnet-base/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/huseinzolkepli/Malaya/alxlnet-model/base/alxlnet-base/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "alxlnet = malaya.transformer.load(model = 'alxlnet')\n",
    "encoder_alxlnet = malaya.summarization.extractive.encoder(alxlnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can load specific domain transformer, eg, sentiment, so our extractive summary more sensitive towards sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Load quantized model will cause accuracy drop.\n",
      "WARNING:root:vectorizer model does not have `attention` method, `top-words` will not work\n"
     ]
    }
   ],
   "source": [
    "alxlnet = malaya.sentiment.transformer(model = 'alxlnet', quantized = True)\n",
    "encoder_sentiment = malaya.summarization.extractive.encoder(alxlnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence level\n",
    "\n",
    "This will predict scores for each sentences,\n",
    "\n",
    "```python\n",
    "def sentence_level(\n",
    "    self,\n",
    "    corpus,\n",
    "    isi_penting: str = None,\n",
    "    top_k: int = 3,\n",
    "    important_words: int = 10,\n",
    "    batch_size: int = 16,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarize list of strings / string on sentence level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: str / List[str]\n",
    "    isi_penting: str, optional (default=None)\n",
    "        if not None, will put priority based on `isi_penting`.\n",
    "    top_k: int, (default=3)\n",
    "        number of summarized strings.\n",
    "    important_words: int, (default=10)\n",
    "        number of important words.\n",
    "    batch_size: int, (default=16)\n",
    "        for each feed-forward, we only feed N size of texts for each batch.\n",
    "        This to prevent OOM.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: {'summary', 'top-words', 'cluster-top-words', 'score'}\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bagi Sheila pula, dia memang ada terbabit dengan beberapa persembahan '\n",
      " 'bersama Zainal cuma tiada publisiti ketika itu. \"Sebab itu, saya sukar '\n",
      " 'menolak untuk bekerjasama dengannya dalam Festival KL Jamm yang dianjurkan '\n",
      " 'buat julung kali dan berkongsi pentas dalam satu konsert bertaraf '\n",
      " 'antarabangsa,\" katanya. Mewakili golongan anak seni, Sheila menaruh harapan '\n",
      " 'semoga Festival KL Jamm akan menjadi platform buat artis yang sudah ada nama '\n",
      " 'dan artis muda untuk membuat persembahan, sekali gus sama-sama memartabatkan '\n",
      " 'industri muzik tempatan.')\n",
      "CPU times: user 400 ms, sys: 107 ms, total: 507 ms\n",
      "Wall time: 347 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = encoder.sentence_level(isu_string, isi_penting = 'antarabangsa')\n",
    "pprint(r['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bagi Sheila pula, dia memang ada terbabit dengan beberapa persembahan '\n",
      " 'bersama Zainal cuma tiada publisiti ketika itu. \"Kami pernah terbabit dengan '\n",
      " 'showcase dan majlis korporat sebelum ini. Selain itu, Zainal juga terbabit '\n",
      " 'dengan Konsert Legenda yang membabitkan jelajah empat lokasi sebelum ini.')\n",
      "CPU times: user 45.9 s, sys: 3.49 s, total: 49.4 s\n",
      "Wall time: 9.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = encoder_alxlnet.sentence_level(isu_string, isi_penting = 'antarabangsa')\n",
    "pprint(r['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"Rata-rata hanya sekadar menyanyi di laman Instagram dan cuma dikenali '\n",
      " 'menerusi satu lagu. Namun, ada satu persamaan yang mengeratkan hubungan '\n",
      " 'mereka kerana sama-sama mencintai bidang muzik sejak dulu. Justeru, '\n",
      " 'bagaimana mereka mahu buat showcase kalau hanya dikenali dengan satu lagu\"?')\n",
      "CPU times: user 23.5 s, sys: 2.54 s, total: 26 s\n",
      "Wall time: 6.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = encoder_sentiment.sentence_level(isu_string, isi_penting = 'antarabangsa')\n",
    "pprint(r['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level\n",
    "\n",
    "This will predict scores for each words. This interface will not returned a summary, just score for each words.\n",
    "\n",
    "```python\n",
    "def word_level(\n",
    "    self,\n",
    "    corpus,\n",
    "    isi_penting: str = None,\n",
    "    window_size: int = 10,\n",
    "    important_words: int = 10,\n",
    "    batch_size: int = 16,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarize list of strings / string on word level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: str / List[str]\n",
    "    isi_penting: str, optional (default=None)\n",
    "        if not None, will put priority based on `isi_penting`.\n",
    "    window_size: int, (default=10)\n",
    "        window size for each word.\n",
    "    important_words: int, (default=10)\n",
    "        number of important words.\n",
    "    batch_size: int, (default=16)\n",
    "        for each feed-forward, we only feed N size of texts for each batch.\n",
    "        This to prevent OOM.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: {'summary', 'top-words', 'cluster-top-words', 'score'}\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 125 ms, total: 2.33 s\n",
      "Wall time: 540 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('DUA', 0.6887927),\n",
       " ('legenda', 0.66629106),\n",
       " ('hebat', 0.68231773),\n",
       " ('dan', 0.7088285),\n",
       " (\"'The\", 0.7100761),\n",
       " ('living', 0.7477336),\n",
       " (\"legend'\", 0.7506831),\n",
       " ('ini', 0.7550354),\n",
       " ('sudah', 0.7749422),\n",
       " ('memartabatkan', 0.68789077),\n",
       " ('bidang', 0.75334096),\n",
       " ('muzik', 0.77209735),\n",
       " ('sejak', 0.77883065),\n",
       " ('lebih', 0.75666404),\n",
       " ('tiga', 0.7756305),\n",
       " ('dekad', 0.77535397),\n",
       " ('lalu.', 0.7996583),\n",
       " ('Jika', 0.7532508),\n",
       " ('Datuk', 0.73633057),\n",
       " ('Zainal', 0.7388078)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = encoder.word_level(isu_string)\n",
    "r['score'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 20s, sys: 21.3 s, total: 3min 41s\n",
      "Wall time: 39.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('DUA', 0.65859354),\n",
       " ('legenda', 0.67562085),\n",
       " ('hebat', 0.6850947),\n",
       " ('dan', 0.6763989),\n",
       " (\"'The\", 0.67380124),\n",
       " ('living', 0.68667483),\n",
       " (\"legend'\", 0.65132356),\n",
       " ('ini', 0.92398083),\n",
       " ('sudah', 0.90694207),\n",
       " ('memartabatkan', 0.9024261),\n",
       " ('bidang', 0.8732655),\n",
       " ('muzik', 0.56828856),\n",
       " ('sejak', 0.24973005),\n",
       " ('lebih', 0.27954298),\n",
       " ('tiga', 0.27266973),\n",
       " ('dekad', 0.83495057),\n",
       " ('lalu.', 0.9397317),\n",
       " ('Jika', 0.9280187),\n",
       " ('Datuk', 0.92466754),\n",
       " ('Zainal', 0.93014336)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = encoder_alxlnet.word_level(isu_string)\n",
    "r['score'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 23.9 s, total: 3min 57s\n",
      "Wall time: 42.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('DUA', 0.7614572),\n",
       " ('legenda', 0.7729454),\n",
       " ('hebat', 0.7851631),\n",
       " ('dan', 0.838418),\n",
       " (\"'The\", 0.88925576),\n",
       " ('living', 0.5610372),\n",
       " (\"legend'\", 0.89473295),\n",
       " ('ini', 0.8995718),\n",
       " ('sudah', 0.87563884),\n",
       " ('memartabatkan', 0.8340343),\n",
       " ('bidang', 0.87825197),\n",
       " ('muzik', 0.867957),\n",
       " ('sejak', 0.8548101),\n",
       " ('lebih', 0.740687),\n",
       " ('tiga', 0.8771104),\n",
       " ('dekad', 0.89566004),\n",
       " ('lalu.', 0.7821031),\n",
       " ('Jika', 0.9377091),\n",
       " ('Datuk', 0.9417584),\n",
       " ('Zainal', 0.8979969)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = encoder_sentiment.word_level(isu_string)\n",
    "r['score'][:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
