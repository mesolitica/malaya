{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a653fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Results generated using stochastic methods. Re-running the in your local might not get the same results.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0894a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "TORCH_DTYPE = 'bfloat16'\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=getattr(torch, TORCH_DTYPE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6daae66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_mistral = AutoTokenizer.from_pretrained('mesolitica/malaysian-mistral-7b-32k-instructions')\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained('mesolitica/malaysian-llama2-7b-32k-instructions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0da5df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = AutoModelForCausalLM.from_pretrained(\n",
    "    'mesolitica/malaysian-mistral-7b-32k-instructions',\n",
    "    use_flash_attention_2 = True,\n",
    "    quantization_config = nf4_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7a7198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = AutoModelForCausalLM.from_pretrained(\n",
    "    'mesolitica/malaysian-llama2-7b-32k-instructions',\n",
    "    use_flash_attention_2 = True,\n",
    "    quantization_config = nf4_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18b9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mistral_chat(messages):\n",
    "    \n",
    "    # mistral does not have system prompt, so we skip\n",
    "    messages = messages[1:]\n",
    "    user_query = messages[-1]['content']\n",
    "\n",
    "    users, assistants = [], []\n",
    "    for q in messages[:-1]:\n",
    "        if q['role'] == 'user':\n",
    "            users.append(q['content'])\n",
    "        elif q['role'] == 'assistant':\n",
    "            assistants.append(q['content'])\n",
    "\n",
    "    texts = ['<s>']\n",
    "    for u, a in zip(users, assistants):\n",
    "        texts.append(f'[INST] {u.strip()} [/INST]{a.strip()}</s> ')\n",
    "\n",
    "    texts.append(f'[INST] {user_query.strip()} [/INST]')\n",
    "    prompt = ''.join(texts).strip()\n",
    "    return prompt\n",
    "\n",
    "def parse_llama_chat(messages):\n",
    "\n",
    "    system = messages[0]['content']\n",
    "    user_query = messages[-1]['content']\n",
    "\n",
    "    users, assistants = [], []\n",
    "    for q in messages[1:-1]:\n",
    "        if q['role'] == 'user':\n",
    "            users.append(q['content'])\n",
    "        elif q['role'] == 'assistant':\n",
    "            assistants.append(q['content'])\n",
    "\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system}\\n<</SYS>>\\n\\n']\n",
    "    for u, a in zip(users, assistants):\n",
    "        texts.append(f'{u.strip()} [/INST] {a.strip()} </s><s>[INST] ')\n",
    "    texts.append(f'{user_query.strip()} [/INST]')\n",
    "    prompt = ''.join(texts).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87f5f0",
   "metadata": {},
   "source": [
    "# OpenQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179bae7e",
   "metadata": {},
   "source": [
    "## KWSP\n",
    "\n",
    "kwsp tu apa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd41430",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91a94745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kumpulan Wang Simpanan Pekerja (KWSP), juga dikenali sebagai Lembaga Kumpulan Wang Simpanan Pekerja, ialah sebuah badan berkanun yang ditubuhkan di bawah undang-undang Malaysia dan mempunyai tanggungjawab untuk mengurus dana persaraan dan simpanan ahli-ahlinya. Dana ini terhasil daripada caruman yang dibuat oleh majikan dan pekerja, serta faedah yang dikumpul daripada pelaburan yang dibuat oleh KWSP. Dana ini ditadbir urus oleh Kumpulan Wang Simpanan Pekerja (KWSP), sebuah agensi kerajaan yang bertanggungjawab untuk menguruskan dana persaraan dan simpanan ahli-ahlinya. KWSP telah ditubuhkan pada tahun 1951 dan telah mengumpul sejumlah besar simpanan melalui pelaburannya, yang menyediakan pendapatan yang stabil dan berterusan untuk ahli. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'kwsp tu apa'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda22cf4",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07881cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KWSP merujuk kepada Kumpulan Wang Simpanan Pekerja, iaitu sebuah badan berkanun yang ditubuhkan di Malaysia yang menguruskan dana simpanan persaraan untuk pekerja sektor swasta.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'kwsp tu apa'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1111d",
   "metadata": {},
   "source": [
    "## Koko\n",
    "\n",
    "awat malaysia ada jabatan koko, malaysia bukan keluaq koko banyak pun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3e3505",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4db5fb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jabatan Koko di Malaysia ditubuhkan pada tahun 1989 untuk mempromosikan dan membangunkan industri koko di negara ini. Industri ini berkembang pesat sejak tahun 1990-an, menjadikan Malaysia salah satu pengeluar utama koko dunia.\n",
      "\n",
      "Jabatan Koko di Malaysia mempunyai misi untuk mempromosikan dan memperkukuhkan industri koko Malaysia melalui kawal selia, perkhidmatan dan aktiviti promosi. Ia bertanggungjawab untuk mengawasi pengeluaran koko, pemprosesan koko dan penggunaan coklat di Malaysia, serta mempromosikan penggunaan produk koko tempatan. Ia juga menggalakkan penyelidikan dan pembangunan dalam industri koko, termasuk penyelidikan ke atas pembiakbakaan dan pengeluaran varieti koko yang berkualiti tinggi.\n",
      "\n",
      "Adalah penting untuk ambil perhatian bahawa Malaysia juga merupakan pengeluar koko utama dunia, dan industri ini telah menyumbang kepada ekonomi negara dan mencipta peluang pekerjaan untuk beribu-ribu orang. Oleh itu, walaupun Malaysia bukan pengeksport utama koko, penanaman koko di Malaysia memainkan peranan penting dalam ekonomi negara. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'awat malaysia ada jabatan koko, malaysia bukan keluaq koko banyak pun'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b6e12",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3324de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jabatan Koko di Malaysia tidak berkaitan dengan pengeluaran coklat, kerana Malaysia tidak menghasilkan koko secara besar-besaran. Sebaliknya, jabatan ini bertanggungjawab untuk mengawal selia industri koko negara, mempromosikan penggunaan koko secara mampan, dan meningkatkan kualiti produk koko tempatan. Walaupun coklat dan koko tidak berkaitan dengan pengeluaran koko, Malaysia mempunyai sejarah dan tradisi yang kaya dalam pengeluaran koko, dan jabatan ini membantu memastikan industri itu terus berkembang maju.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'awat malaysia ada jabatan koko, malaysia bukan keluaq koko banyak pun'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20577cb",
   "metadata": {},
   "source": [
    "# Code instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b654fde",
   "metadata": {},
   "source": [
    "## Kill PID\n",
    "\n",
    "camne nak kill pid dalam linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113df08a",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dc0f182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pada Linux, \"kill\" adalah alat yang membolehkan anda menamatkan sesuatu proses. Untuk menamatkan proses ID tertentu, anda boleh menggunakan arahan \"kill -p\".\n",
      "\n",
      "Contohnya:\n",
      "```\n",
      "kill -p 8483\n",
      "```\n",
      "\n",
      "Ini akan membunuh proses dengan nombor ID 8483. Jika anda mempunyai lebih banyak maklumat tentang proses itu, anda boleh menggunakan arahan berikut:\n",
      "```\n",
      "kill -p 8483\n",
      "```\n",
      "\n",
      "Ini akan mencari proses dengan nombor ID 8483 dan membunuhnya, tanpa mengira jenis proses itu. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'camne nak kill pid dalam linux'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755eca06",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11026532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terdapat beberapa cara untuk membunuh pid dalam Linux:\n",
      "\n",
      "1. Gunakan perintah `pgrep`: Ini adalah alat yang disertakan dengan Linux yang boleh digunakan untuk mencari pid. Jalankan perintah `pgrep nama_pid_yang_dikehendaki` di mana `nama_pid_yang_dikehendaki` adalah nama pid yang ingin anda bunuh. Ini akan mengembalikan PID (ID proses) untuk pid. Kemudian, jalankan perintah `kill` dengan PID untuk membunuhnya.\n",
      "\n",
      "2. Gunakan perintah `ps`: Ini juga disertakan dengan Linux dan mengembalikan senarai proses yang sedang berjalan. Cari PID yang ingin anda bunuh, kemudian gunakan perintah `kill` untuk membunuhnya.\n",
      "\n",
      "3. Gunakan perintah `pkill`: Ini adalah perintah yang lebih kuat yang digunakan untuk membunuh satu atau lebih proses. Gunakan perintah `pkill` diikuti dengan nama proses yang ingin anda bunuh untuk membunuhnya.\n",
      "\n",
      "4. Gunakan perintah `systemctl`: Ini digunakan untuk menguruskan perkhidmatan sistem yang dijalankan secara automatik oleh kernel. Gunakan perintah `systemctl` untuk membunuh perkhidmatan yang bertanggungjawab terhadap proses yang ingin anda bunuh.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'camne nak kill pid dalam linux'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024724f5",
   "metadata": {},
   "source": [
    "## htmx websocket\n",
    "\n",
    "contoh htmx websocket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1777f9d0",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4abb92ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks XML contoh web socket contoh:\n",
      "\n",
      "```html\n",
      "<ws:http:11020, \"https://api.example.com/websocket/example/data\"\n",
      "HTTP/1.1\n",
      "\"GET\", \"GET\"\n",
      "\"https://api.example.com/websocket/example/data\"\n",
      "\"200, 200, 200\"\n",
      "\"Pemulihan\"\n",
      "\"Mesej\"\n",
      "\"Panggilan\"\n",
      "\"Pemanggilan\"\n",
      "\"Bersiaran\"\n",
      "```\n",
      "\n",
      "Ia menentukan cara untuk mengesan kehadiran web socket pada pelayan web dan mengesahkan panggilan klien. Ia menyatakan bahawa pemanggilan web socket mendapat semula balasan 200 dan klien yang menanggapi akan menghantar mesej yang dinyatakan pada kandungan. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'contoh htmx websocket'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274107b5",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4efaf03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saya tidak dapat memberikan contoh kod kerana saya tidak mempunyai akses kepada bahasa pengaturcaraan yang diperlukan. walau bagaimanapun, saya boleh memberikan anda gambaran keseluruhan konsep di sebalik websocket dan bagaimana ia berfungsi.\n",
      "\n",
      "websocket ialah protokol komunikasi yang menggunakan teknologi wwdc/networking transport layer (tcp/ip) untuk mewujudkan sambungan yang pantas dan boleh dipercayai antara pelanggan dan pelayan dalam persekitaran web. ia melibatkan penghantaran data binari (seperti imej, video dan audio) ke dan dari pelayar atau peranti lain dan boleh digunakan untuk komunikasi masa nyata antara pelayar dan pelayan, serta antara pelayan dan berbilang pelanggan.\n",
      "\n",
      "websocket berfungsi dengan menghantar mesej yang dipanggil \"paket\" ke/dari pelayan melalui sambungan tcp/ip. pelanggan kemudiannya boleh menggunakan paket ini untuk berkomunikasi dengan pelayan atau pelanggan lain. soket boleh ditutup pada bila-bila masa, dan data yang dihantar dalam paket akan diterima dan diproses secara automatik oleh pelanggan.\n",
      "\n",
      "berikut adalah contoh bagaimana anda boleh menggunakan websocket untuk menghantar mesej daripada klien ke pelayan:\n",
      "\n",
      "1. cipta objek websocket dalam klien anda dan sambungkan ke pelayan:\n",
      "\n",
      "```javascript\n",
      "var ws = new websocket('/path/to/server');\n",
      "ws.onopen = function () {\n",
      "  console.log(\"websocket connected!\");\n",
      "}\n",
      "```\n",
      "\n",
      "2. cipta mesej yang anda ingin hantar ke pelayan:\n",
      "\n",
      "```javascript\n",
      "var message = 'hi there';\n",
      "```\n",
      "\n",
      "3. hantar mesej ke pelayan:\n",
      "\n",
      "```javascript\n",
      "ws.send(message);\n",
      "```\n",
      "\n",
      "4. dalam kod pelayan anda, cipta objek websocket dan dengar paket:\n",
      "\n",
      "```javascript\n",
      "var ws = new websocket('/path/to/client');\n",
      "\n",
      "ws.onmessage = function (e) {\n",
      "  console.log(e.data);\n",
      "}\n",
      "```\n",
      "\n",
      "5. dalam pelayan anda, anda boleh menutup soket pada bila-bila masa:\n",
      "\n",
      "```javascript\n",
      "ws.close();\n",
      "```\n",
      "\n",
      "6. akhirnya, dalam klien anda, anda boleh menutup soket pada bila-bila masa:\n",
      "\n",
      "```javascript\n",
      "ws.close();\n",
      "```\n",
      "\n",
      "secara keseluruhan, websocket ialah protokol yang berkuasa dan serba boleh yang boleh digunakan untuk komunikasi masa nyata dalam aplikasi web.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'contoh htmx websocket'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29062218",
   "metadata": {},
   "source": [
    "## Ruby on Rails\n",
    "\n",
    "mcam mane nak start ruby on rails, bagi contoh code sekali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da5cf9",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f3d2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anda boleh memulakan Ruby on Rails dengan mengikuti langkah berikut:\n",
      "\n",
      "1. Pasang Ruby on Rails dengan memuat turun dan memasang versi terkini Ruby on Rails dari laman web rasmi Ruby on Rails.\n",
      "2. Buka terminal dan jalankan arahan berikut:\n",
      "\n",
      "```\n",
      "gem install ruby-on-rails\n",
      "```\n",
      "\n",
      "Ini akan memuat turun dan memasang Ruby on Rails ke dalam sistem anda.\n",
      "3. Cipta panduan projek Ruby on Rails baru dan namakannya mengikut pilihan anda.\n",
      "4. Cipta fail untuk projek anda di direktori yang anda ingin simpan projek anda.\n",
      "5. Selesaikan fail projek anda, biasanya menggunakan program penyunting teks seperti Sublime, Textual, atau VIM.\n",
      "6. Mulakan aplikasi Ruby on Rails dengan memuat turun dan memasang Ruby on Rails daripada tapak web Ruby on Rails.\n",
      "7. Ujilah aplikasi anda untuk memastikan ia berfungsi dengan betul dan memperbetulkan sebarang ralat yang dijumpai.\n",
      "8. Kemas kini pakej Ruby on Rails anda dengan menggunakan arahan berikut:\n",
      "```\n",
      "gem update\n",
      "```\n",
      "\n",
      "Ini akan memuat turun dan memasang pakej Ruby on Rails terkini dan mengemas kini mana-mana ketidaksempurnaan.\n",
      "9. Laraskan konfigurasi Ruby on Rails untuk memenuhi keperluan khusus projek anda.\n",
      "10. Mulakan perniagaan anda menggunakan aplikasi Ruby on Rails anda, dan teruskan mengembangkannya dan meningkatkan kefungsiannya dengan menggunakan sumber, alatan dan dokumentasi Ruby on Rails yang tersedia. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'mcam mane nak start ruby on rails, bagi contoh code sekali'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45a4bc",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee868f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuk memulakan Ruby on Rails, anda perlu membuat aplikasi web mudah terlebih dahulu. Berikut adalah contoh kod untuk aplikasi web yang mudah:\n",
      "\n",
      "```\n",
      "# Gemfile\n",
      "gem 'rails'\n",
      "\n",
      "# Fail konfigurasi aplikasi\n",
      "config/application.rb\n",
      "rails.application :app_name => 'hello_world'\n",
      "rails.application :user => 'hello_world'\n",
      "rails.application :port => 3000\n",
      "```\n",
      "\n",
      "Dalam fail ini, anda telah menentukan beberapa maklumat asas tentang aplikasi anda seperti nama aplikasi, nama pengguna, dan port.\n",
      "\n",
      "Aplikasi ini akan mempunyai dua laluan, `/hello_world` dan `/world`. Jika anda mengakses laman web melalui URL-URL tersebut, anda akan melihat mesej \"Hello World!\".\n",
      "\n",
      "Untuk menguji aplikasi anda, anda boleh mengaksesnya melalui URL http://localhost:3000.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'mcam mane nak start ruby on rails, bagi contoh code sekali'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc77793",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "\n",
    "camne nak train sentiment analysis guna tfidf bayes sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b67a5",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24179518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dalam TensorFlow, anda boleh menggunakan `tf.tfidf.multinomial_dist.MultinomialProbabilityDistribution` untuk melatih model pengagregatan kebarangkalian berasaskan Bayes. Anda perlu mengambil dua kelas data dengan tahap berbeza sebagai input dan menjana set data yang tidak diberikan yang mana anda mahu model itu diramalkan. Anda kemudiannya boleh menilai ramalan dan menggunakan outputnya untuk melatih model. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'camne nak train sentiment analysis guna tfidf bayes sklearn'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15dbf06",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96c5317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuk melatih analisis sentimen menggunakan TF-IDF Bayes dalam scikit-learn, anda boleh mengikuti langkah-langkah berikut:\n",
      "\n",
      "1. Muatkan dataset anda ke dalam Pandas DataFrame.\n",
      "\n",
      "```\n",
      "from pandas import DataFrame\n",
      "\n",
      "df = pd.DataFrame(data, columns=columns)\n",
      "```\n",
      "\n",
      "2. Latih pengelas anda menggunakan TF-IDF Bayes dengan scikit-learn.\n",
      "\n",
      "```\n",
      "from sklearn.feature_extraction.text import TFIDFVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "vectorizer = TFIDFVectorizer(stop_words='english')\n",
      "classifier = MultinomialNB()\n",
      "classifier.fit(x=df['text'], y=df['sentiment'])\n",
      "```\n",
      "\n",
      "3. Gunakan pengelas terlatih untuk meramalkan sentimen dataset baru.\n",
      "\n",
      "```\n",
      "new_df = pd.DataFrame({\n",
      "    'text': ['Ini adalah teks positif.',\n",
      "    'Ini adalah teks neutral.',\n",
      "    'Ini adalah teks negatif.',\n",
      "    'Ini adalah teks neutral.'\n",
      "})\n",
      "\n",
      "predicted_sentiment = classifier.predict(new_df['text'])\n",
      "print(predicted_sentiment)\n",
      "```\n",
      "\n",
      "Output akan menjadi skor sentimen yang diramalkan untuk setiap baris dalam dataset baru. Anda boleh mengubah kamus `data` dan menyesuaikan kelas `stop_words` untuk mengendalikan bahasa yang berbeza.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'camne nak train sentiment analysis guna tfidf bayes sklearn'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c085664",
   "metadata": {},
   "source": [
    "## SQL to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c96c7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "CREATE TABLE pelajar (\n",
    "   id SERIAL PRIMARY KEY,\n",
    "   name VARCHAR(50),\n",
    "   email VARCHAR(50),\n",
    "   student_number VARCHAR(50),\n",
    "   course VARCHAR(50),\n",
    "   start_date DATE,\n",
    "   end_date DATE,\n",
    "   grade VARCHAR(50)\n",
    ")\n",
    "\n",
    "berdasarkan table definition, bina sql query untuk filter pelajar yang grade C keatas\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c645409",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49fc04d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT name FROM pelajar WHERE grade > 'C' </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': s}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2f754",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2f8675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM pelajar WHERE grade >= \"C\"</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': s}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6db18",
   "metadata": {},
   "source": [
    "# Mathematics\n",
    "\n",
    "From https://huggingface.co/datasets/camel-ai/math, translate using google translate.\n",
    "\n",
    "1. Cari bucu bagi fungsi nilai mutlak f(x) = |2x - 5|.\n",
    "2. Dalam kumpulan 6 orang, setiap orang berkawan dengan sekurang-kurangnya 3 orang lain. Berapakah bilangan minimum kumpulan 3 orang yang perlu dibentuk untuk memastikan sekurang-kurangnya terdapat satu kumpulan yang kesemua ahli berkawan antara satu sama lain?\n",
    "3. Cari persamaan satah yang mengandungi persilangan permukaan:\\n  $x^2 + y^2 + z^2 = 25$ dan $x - y + z = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999dc83",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "\n",
    "Untuk mencari bucu bagi fungsi nilai mutlak f(x) = |2x - 5|, kita perlu mencari koordinat-x bagi puncak tersebut. Puncak berlaku pada titik di mana ungkapan di dalam nilai mutlak adalah sama dengan sifar.\\n\\n2x - 5 = 0\\n\\nSekarang, selesaikan untuk x:\\n\\n2x = 5\\nx = 5/2\\n\\nKoordinat-x bagi bucu ialah 5/2. Sekarang, kita perlu mencari koordinat-y yang sepadan dengan memasukkan semula koordinat-x ke dalam fungsi:\\n\\nf(5/2) = |2(5/2) - 5|\\n\\nf(5/2) = |5 - 5|\\n\\nf(5/2) = 0\\n\\nJadi, koordinat y bagi bucu ialah 0. Oleh itu, bucu bagi fungsi nilai mutlak f(x) = |2x - 5| ialah (5/2, 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d93fd",
   "metadata": {},
   "source": [
    "## Answer 2\n",
    "\n",
    "Mari kita nyatakan orang sebagai A, B, C, D, E, dan F. Kita boleh mewakili persahabatan antara mereka menggunakan graf, di mana setiap orang adalah bucu dan tepi antara dua bucu mewakili persahabatan.\\n\\nMemandangkan setiap orang berkawan dengan sekurang-kurangnya 3 orang lain, kita boleh mulakan dengan menyambungkan A ke B, C dan D. Sekarang, kita perlu memastikan B, C dan D mempunyai sekurang-kurangnya 3 rakan juga. Jika B berkawan dengan kedua-dua C dan D, maka kami sudah mempunyai kumpulan 3 orang yang semuanya berkawan antara satu sama lain (A, B, dan C atau A, B, dan D). Jadi, untuk meminimumkan bilangan kumpulan sedemikian, kita boleh menganggap bahawa B tidak berkawan dengan C atau D. Sebaliknya, B boleh berkawan dengan E dan F.\\n\\nKini, kami mempunyai persahabatan berikut: A-B, A-C, A-D, B-E dan B-F. Kita masih perlu memastikan C, D, E dan F mempunyai sekurang-kurangnya 3 rakan setiap satu. Jika C berkawan dengan D, maka kami mempunyai sekumpulan 3 orang yang semuanya berkawan antara satu sama lain (A,C, dan D). Jadi, untuk meminimumkan bilangan kumpulan sedemikian, kita boleh menganggap bahawa C tidak berkawan dengan D. Sebaliknya, C boleh berkawan dengan E dan F.\\n\\nKini, kami mempunyai persahabatan berikut: A-B, A-C, A-D, B-E, B-F, C-E dan C-F. Kita masih perlu memastikan D, E, dan F mempunyai sekurang-kurangnya 3 rakan setiap satu. Jika D berkawan dengan E atau F, maka kami mempunyai sekumpulan 3 orang yang semuanya berkawan antara satu sama lain (A, D, dan E atau A, D, dan F). Jadi, untuk meminimumkan bilangan kumpulan sedemikian, kita boleh menganggap bahawa D tidak berkawan dengan E atau F. Sebaliknya, D boleh berkawan dengan G.\\n\\nWalau bagaimanapun, kami hanya mempunyai 6 orang dalam kumpulan, jadi kami tidak boleh menambah orang baru G. Oleh itu, D mesti berkawan dengan sama ada E atau F. Tanpa kehilangan sifat umum, mari kita anggap D berkawan dengan E. Sekarang, kita mempunyai kumpulan 3 orang yang semuanya berkawan antara satu sama lain (A, D, dan E).\\n\\nJusteru, bilangan minimum kumpulan 3 orang yang perlu dibentuk bagi memastikan sekurang-kurangnya terdapat satu kumpulan yang kesemua ahli berkawan antara satu sama lain ialah 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26acc5aa",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f09e076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nilai bagi fungsi nilai mutlak f(x) = |2x - 5| ialah sama dengan |2x - 5| iaitu x = 3. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'Cari bucu bagi fungsi nilai mutlak f(x) = |2x - 5|.'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46d65cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 kumpulan </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'Dalam kumpulan 6 orang, setiap orang berkawan dengan sekurang-kurangnya 3 orang lain. Berapakah bilangan minimum kumpulan 3 orang yang perlu dibentuk untuk memastikan sekurang-kurangnya terdapat satu kumpulan yang kesemua ahli berkawan antara satu sama lain?'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0346bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persamaan satah yang mengandungi persilangan permukaan:\n",
      "\n",
      "$x^2 + y^2 + z^2 = 25$ dan $x - y + z = 0$ ialah\n",
      "\n",
      "```\n",
      "x1 + y1 + z1 = 25\n",
      "x2 + y2 + z2 = 25\n",
      "x1 + x2 + y1 + y2 + z1 + z2 = 25\n",
      "```\n",
      "\n",
      "Dalam bentuk matriks, persamaan satah ialah\n",
      "\n",
      "```\n",
      "A =\n",
      "  [1 0 0 -x1 - y1 - z1]\n",
      "  [1 1 -x1 - y1]\n",
      "  [1 -x2 - y2]\n",
      "  [1 1 -x1 - y1 - z2]\n",
      "  [1 -x2 - y2]\n",
      "```\n",
      "\n",
      "Dalam bentuk matriks, persilangan permukaan ialah\n",
      "\n",
      "```\n",
      "B =\n",
      "  [1]\n",
      "  [0 -x1 - y1]\n",
      "  [-1, -x2 - y2]\n",
      "  [1]\n",
      "  [-1, -x2 - y2]\n",
      "  [1 -x1 - y1 - z2]\n",
      "  [1 -x1 - y1 - z2]\n",
      "```\n",
      "\n",
      "Dalam bentuk matriks, persilangan permukaan ialah\n",
      "\n",
      "```\n",
      "AxB =\n",
      "  [1, 0, -x1, 1, x2, -x2, y1]\n",
      "  [0, -x1, 1, -x2, -x2, -y2, -x1]\n",
      "  [1, -x1, -x2, -x2, -y2, -y2, -x1]\n",
      "  [1, -x1, -x2, -x2, -y2, -y2, -x1]\n",
      "```\n",
      "\n",
      "Nota: Matriks A ialah matriks yang mengandungi persamaan satah dan persilangan permukaan ialah matriks yang diperoleh daripada mengendalikan A terhadap faktor penentu -1. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'Cari persamaan satah yang mengandungi persilangan permukaan:\\n  $x^2 + y^2 + z^2 = 25$ dan $x - y + z = 0$.'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6449c2d",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73da0ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fungsi nilai mutlak mempunyai domain semua nombor sebenar dan hanya ditakrifkan untuk x sebenar.\n",
      "Untuk mencari puncak, kita perlu mencari nilai maksimum atau minimum fungsi nilai mutlak.\n",
      "Mengambil derivatif berkenaan dengan x, kita mempunyai:\n",
      "f'(x) = 2 - 2x + 5 - 2x + 5x + 25\n",
      "= 2x - 5\n",
      "\n",
      "Menetapkan f'(x) sama dengan 0 dan menyelesaikan x, kita dapat:\n",
      "2x - 5 = 0\n",
      "x = 5/2\n",
      "\n",
      "Oleh kerana terdapat lebih banyak nilai dalam domain berbanding sempadan, puncak adalah lebih rendah daripada sempadan.\n",
      "Oleh itu, puncak fungsi nilai mutlak ialah $\\boxed{(-\\frac{5}{2}, 5/2)}$.\n",
      "Jawapannya ialah: (-\\frac{5}{2},5/2)</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'Cari bucu bagi fungsi nilai mutlak f(x) = |2x - 5|.'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4b522fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuk mencari bilangan minimum 3 orang yang diperlukan untuk membentuk kumpulan, kita boleh menggunakan konsep prinsip mengira.\n",
      "Jika terdapat 6 orang dalam kumpulan, maka terdapat 6C3=180 cara untuk memilih 3 orang daripada kumpulan.\n",
      "Jika setiap orang berkawan dengan sekurang-kurangnya 3 orang lain, maka bilangan kumpulan 3 orang mestilah sekurang-kurangnya 180, kerana terdapat 6 orang.\n",
      "Untuk memastikan bahawa tiada dua kumpulan berkongsi semua ahli, kita perlu mempunyai sekurang-kurangnya 180-1=179 ahli dalam kumpulan.\n",
      "Oleh kerana setiap ahli mesti berada dalam satu atau lebih kumpulan, kita mesti mempunyai sekurang-kurangnya 179 ahli dalam kumpulan.\n",
      "Oleh itu, bilangan minimum kumpulan 3 orang yang perlu dibentuk untuk memastikan sekurang-kurangnya terdapat satu kumpulan yang kesemua ahlinya berkawan antara satu sama lain ialah $\\boxed{180}$.\n",
      "Jawapannya ialah: 180</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'Dalam kumpulan 6 orang, setiap orang berkawan dengan sekurang-kurangnya 3 orang lain. Berapakah bilangan minimum kumpulan 3 orang yang perlu dibentuk untuk memastikan sekurang-kurangnya terdapat satu kumpulan yang kesemua ahli berkawan antara satu sama lain?'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "612ebb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kita boleh menulis semula persamaan satah dalam sebutan koordinat sebagai $(x + y)^2 + (x + z)^2 = 25$.\n",
      "Ini ialah persamaan elips, dan bentuknya ialah $(x + y)^2 + (x + z)^2 = \\frac{25}{4}$.\n",
      "Membandingkan pekali, kita mempunyai $c = \\frac{25}{4}$, iaitu jarak dari asal ke satah dalam satah Cartesan.\n",
      "Persamaan satah ini diberikan oleh $x^2 + y^2 + z^2 = \\frac{25}{4} + \\frac{25}{4}$.\n",
      "Untuk mencari persamaan satah, kita boleh menulis semula persamaan ini sebagai $(x + y)^2 + (x + z)^2 = \\frac{50}{4}$, yang boleh ditulis sebagai $(x + y)^2 + (x + z)^2 = \\frac{10}{2}$.\n",
      "Ini ialah persamaan hiperbola, dan bentuknya ialah $(x + y)^2 + (x + z)^2 = \\frac{10}{2}$.\n",
      "Persamaan satah yang mengandungi titik ini ialah $\\frac{10}{2} = \\frac{x^2 + y^2 + z^2}{2} = \\frac{10}{2}$.\n",
      "Mendarab kedua-dua belah dengan 2 memberikan $10 = x^2 + y^2 + z^2$.\n",
      "Jadi persamaan satah ialah $x^2 + y^2 + z^2 = \\boxed{10}$.\n",
      "Jawapannya ialah: 10</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'Cari persamaan satah yang mengandungi persilangan permukaan:\\n  $x^2 + y^2 + z^2 = 25$ dan $x - y + z = 0$.'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b08d8",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "1. SHAH ALAM - Pertubuhan Kebajikan Anak Bersatu Selangor bersetuju pihak kerajaan mewujudkan Suruhanjaya Siasatan Diraja untuk menyiasat isu kartel daging. to English\n",
    "2. Smh the dude literally has a rubbish bin infront of his house. to standard Malay\n",
    "3. Syed Saddiq berkata, mereka seharusnya mengingati bahawa semasa menjadi Perdana Menteri Pakatan Harapan. to English\n",
    "4. Kalau yang rasa overwhelming pasal belajar CSS tu, ini 7 fasa untuk anda belajar step-by-step. 5 fasa pertama tu wajib. Cuma tak perlu mahir semua dalam setiap fasa. Dengan sedikit pemahaman dah boleh mula ke fasa seterusnya, mengikut keperluan masing2, to standard Malay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771cb450",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8902230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAH ALAM - The Welfare Association of United Children of Selangor agreed that the government should establish a Royal Commission of Inquiry to investigate the issue of meat cartels. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'translate to English `SHAH ALAM - Pertubuhan Kebajikan Anak Bersatu Selangor bersetuju pihak kerajaan mewujudkan Suruhanjaya Siasatan Diraja untuk menyiasat isu kartel daging.`'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c66d5806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saya sedih kerana dia sebenarnya mempunyai tong sampah di hadapan rumahnya. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'translate to standard Malay `Smh the dude literally has a rubbish bin infront of his house`'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62a012dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syed Saddiq said that they should remember that when Pakatan Harapan was in power, he was the Prime Minister. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'translate to English `Syed Saddiq berkata, mereka seharusnya mengingati bahawa semasa menjadi Perdana Menteri Pakatan Harapan`'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d60e8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jika anda merasa bingung tentang mempelajari CSS, ini adalah 7 tahap untuk mempelajari langkah demi langkah. Hanya 5 tahap pertama diperlukan. Walaupun tidak perlu mahir dalam setiap fasa, dengan sedikit pemahaman, anda boleh mulai ke fasa berikutnya, bergantung pada keperluan anda. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'translate to standard Malay `Kalau yang rasa overwhelming pasal belajar CSS tu, ini 7 fasa untuk anda belajar step-by-step. 5 fasa pertama tu wajib. Cuma tak perlu mahir semua dalam setiap fasa. Dengan sedikit pemahaman dah boleh mula ke fasa seterusnya, mengikut keperluan masing2`'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de29f37",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b3ba3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAH ALAM - The People's Welfare Organization of Selangor agreed to the government creating a Royal Commission of Inquiry to investigate the meat cartel issue.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'translate to English `SHAH ALAM - Pertubuhan Kebajikan Anak Bersatu Selangor bersetuju pihak kerajaan mewujudkan Suruhanjaya Siasatan Diraja untuk menyiasat isu kartel daging.`'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f8adddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggelengkan kepala, orang itu benar-benar mempunyai tong sampah yang sangat buruk di depannya.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'translate to standard Malay `Smh the dude literally has a rubbish bin infront of his house`'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dddd099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syed Saddiq stated that they should remember that when they were Prime Minister of Pakatan Harapan</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'translate to English `Syed Saddiq berkata, mereka seharusnya mengingati bahawa semasa menjadi Perdana Menteri Pakatan Harapan`'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29fea2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jika anda berasa terlalu terharu semasa belajar CSS, berikut adalah 7 fasa yang perlu anda pelajari langkah demi langkah. 5 fasa pertama adalah wajib. Tidak perlu mahir dalam setiap fasa, tetapi cukup untuk memahami asasnya sebelum meneruskan ke fasa seterusnya. Sesuaikan pembelajaran anda dengan keperluan anda.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': 'translate to standard Malay `Kalau yang rasa overwhelming pasal belajar CSS tu, ini 7 fasa untuk anda belajar step-by-step. 5 fasa pertama tu wajib. Cuma tak perlu mahir semua dalam setiap fasa. Dengan sedikit pemahaman dah boleh mula ke fasa seterusnya, mengikut keperluan masing2`'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34c185",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "```\n",
    "KUALA LUMPUR: Perdana Menteri menegaskan terdapat had tertentu yang perlu dipatuhi oleh kerajaan dalam isu kewarganegaraan kerana ia melibatkan Majlis Raja-Raja.\n",
    "\n",
    "Datuk Seri Anwar Ibrahim berkata, kerajaan perlu mengambil kira dan menghormati pandangan Majlis Raja-Raja berhubung isu pemberian kerakyatan tersebut.\n",
    "\n",
    "Kita akan maklumkan kepada mereka (Ahli Parlimen) kerana ini telahpun dibentangkan kepada Majlis Raja-Raja. Terdapat had mengenai apa yang kita boleh lakukan selepas perbincangan (dilakukan).\n",
    "\n",
    "Persidangan Majlis Raja-Raja sudah mengambil pendirian (mengenai isu kewarganegaraan), jadi kita perlu menghormatinya, katanya kepada media selepas melancarkan Ekspo Digital Malaysia (MDX) 2023 di Pusat Pameran Perdagangan Antarabangsa Malaysia (MITEC).\n",
    "\n",
    "Turut hadir Menteri Komunikasi dan Digital, Fahmi Fadzil dan Timbalannya, Teo Nie Ching.\n",
    "\n",
    "Terdahulu media melaporkan, seorang Ahli Parlimen DAP memberitahu terdapat kemungkinan ahli-ahli Parlimen tidak sefahaman dan akan menentang arahan Ketua Whip di Dewan Rakyat, jika kerajaan tidak menangani dengan baik isu lapan cadangan pindaan Perlembagaan Persekutuan berkaitan kewarganegaraan.  \n",
    "```\n",
    "\n",
    "from https://www.utusan.com.my/nasional/2023/11/isu-kewarganegaraan-kerajaan-perlu-hormat-pandangan-majlis-raja-raja/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd38f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "KUALA LUMPUR: Perdana Menteri menegaskan terdapat had tertentu yang perlu dipatuhi oleh kerajaan dalam isu kewarganegaraan kerana ia melibatkan Majlis Raja-Raja.\n",
    "\n",
    "Datuk Seri Anwar Ibrahim berkata, kerajaan perlu mengambil kira dan menghormati pandangan Majlis Raja-Raja berhubung isu pemberian kerakyatan tersebut.\n",
    "\n",
    "Kita akan maklumkan kepada mereka (Ahli Parlimen) kerana ini telahpun dibentangkan kepada Majlis Raja-Raja. Terdapat had mengenai apa yang kita boleh lakukan selepas perbincangan (dilakukan).\n",
    "\n",
    "Persidangan Majlis Raja-Raja sudah mengambil pendirian (mengenai isu kewarganegaraan), jadi kita perlu menghormatinya, katanya kepada media selepas melancarkan Ekspo Digital Malaysia (MDX) 2023 di Pusat Pameran Perdagangan Antarabangsa Malaysia (MITEC).\n",
    "\n",
    "Turut hadir Menteri Komunikasi dan Digital, Fahmi Fadzil dan Timbalannya, Teo Nie Ching.\n",
    "\n",
    "Terdahulu media melaporkan, seorang Ahli Parlimen DAP memberitahu terdapat kemungkinan ahli-ahli Parlimen tidak sefahaman dan akan menentang arahan Ketua Whip di Dewan Rakyat, jika kerajaan tidak menangani dengan baik isu lapan cadangan pindaan Perlembagaan Persekutuan berkaitan kewarganegaraan.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013efc2",
   "metadata": {},
   "source": [
    "### Llama2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b42bc094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perdana Menteri menegaskan terdapat had tertentu yang perlu dipatuhi oleh kerajaan dalam isu kewarganegaraan kerana ia melibatkan Majlis Raja-Raja. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': f'paragraph `{context}`, isu apa ni'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eaaa51d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ekspo Digital Malaysia (MDX) 2023 di Pusat Pameran Perdagangan Antarabangsa Malaysia (MITEC) </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': f'paragraph `{context}`, apa yang dilancarkan'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7cece66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perdana Menteri menegaskan bahawa kerajaan perlu mengambil kira dan menghormati pandangan Majlis Raja-Raja berhubung isu pemberian kerakyatan. </s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': f'paragraph `{context}`, sapa yang mai dalam pelancaran tu'}\n",
    "]\n",
    "prompt = parse_llama_chat(messages)\n",
    "inputs = tokenizer_llama([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = llama.generate(**generate_kwargs)\n",
    "print(tokenizer_llama.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ffee1",
   "metadata": {},
   "source": [
    "### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "22ef74d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isu yang dibincangkan adalah had tertentu yang perlu dipatuhi oleh kerajaan dalam isu kewarganegaraan kerana ia melibatkan Majlis Raja-Raja.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': f'paragraph `{context}`, isu apa ni'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e1aaf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datuk Seri Anwar Ibrahim telah melancarkan Ekspo Digital Malaysia (MDX) 2023 di Pusat Pameran Perdagangan Antarabangsa Malaysia (MITEC).</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': f'paragraph `{context}`, apa yang dilancarkan'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4aaf8569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menteri Komunikasi dan Digital, Fahmi Fadzil dan Timbalannya, Teo Nie Ching.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'awak adalah AI yang mampu jawab segala soalan'},\n",
    "    {'role': 'user', 'content': f'paragraph `{context}`, sapa yang mai dalam pelancaran tu'}\n",
    "]\n",
    "prompt = parse_mistral_chat(messages)\n",
    "inputs = tokenizer_mistral([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")\n",
    "r = mistral.generate(**generate_kwargs)\n",
    "print(tokenizer_mistral.decode(r[0]).split('[/INST]')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669e88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
