{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finetune-t5-base-noisy-bahasa-cased/checkpoint-230000',\n",
       " 'finetune-t5-base-noisy-bahasa-cased/checkpoint-240000',\n",
       " 'finetune-t5-base-noisy-bahasa-cased/checkpoint-250000',\n",
       " 'finetune-t5-base-noisy-bahasa-cased/checkpoint-260000',\n",
       " 'finetune-t5-base-noisy-bahasa-cased/checkpoint-270000']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "checkpoints = sorted(glob('finetune-t5-base-noisy-bahasa-cased/checkpoint-*'))\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('mesolitica/t5-small-standard-bahasa-cased')\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoints[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Hi guys! I noticed yesterday and today many of these cookies are available. So today I want to share some post mortem of our first batch:</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Melayu ke Inggeris: Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Hai kawan! Saya perhatikan semalam & harini ramai yang dapat cookies ni kan. Jadi harini saya nak kongsi beberapa post mortem kumpulan pertama kami:</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Inggeris ke Melayu: Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> I don't understand la.</s>\n",
      "<pad> At 8 a.m., the market is a lot of people, so he's a good person.</s>\n",
      "<pad> So it's fucking shit.</s>\n",
      "<pad> Where are you going?</s>\n",
      "<pad> It's like taking half a day.</s>\n",
      "<pad> Imagine PH and win pru-14. Passovers are all kinds of back doors. Last-last Ismail Sabri goes up. That's why I don't give a fuck about politics anymore. I swear I'm up.</s>\n"
     ]
    }
   ],
   "source": [
    "strings = [\n",
    "    'ak tak paham la',\n",
    "    'jam 8 di pasar KK memang org ramai üòÇ, pandai dia pilih tmpt.',\n",
    "    'Jadi haram jadahüòÄüòÉü§≠',\n",
    "    'nak gi mana tuu',\n",
    "    'Macam nak ambil half day',\n",
    "    \"Bayangkan PH dan menang pru-14. Pastu macam-macam pintu belakang ada. Last-last Ismail Sabri naik. That's why I don't give a fk about politics anymore. Sumpah dah fk up dah.\",\n",
    "]\n",
    "for s in strings:\n",
    "    input_ids = tokenizer.encode(f'terjemah Melayu ke Inggeris: {s}', return_tensors = 'pt')\n",
    "    outputs = model.generate(input_ids, max_length = 100, )\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> u ni, cakap betul lah</s>\n",
      "<pad> pelik jugak dia buat majlis biasa2 je sebab gaya hidup dia dah mewah...................................................................................\n",
      "<pad> Selepas menonton video ini: mm dapnya burger benjo extra mayo</s>\n",
      "<pad> Hai kawan! Saya perhatikan semalam & harini ramai yang dapat cookies ni kan. Jadi harini saya nak kongsi beberapa post mortem kumpulan pertama kami:</s>\n"
     ]
    }
   ],
   "source": [
    "strings = [\n",
    "    'u ni, talk properly lah',\n",
    "    \"just attended my cousin's wedding. pelik jugak dia buat majlis biasa2 je sebab her lifestyle looks lavish. then i found out they're going on a 3 weeks honeymoon. smart decision üëç\",\n",
    "    'Me after seeing this video: mm dapnya burger benjo extra mayo',\n",
    "    'Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:',\n",
    "]\n",
    "for s in strings:\n",
    "    input_ids = tokenizer.encode(f'terjemah Inggeris ke Melayu: {s}', return_tensors = 'pt')\n",
    "    outputs = model.generate(input_ids, max_length = 100)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.push_to_hub('finetune-noisy-translation-t5-base-bahasa-cased', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub('finetune-noisy-translation-t5-base-bahasa-cased', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r finetune-t5-base-noisy-bahasa-cased/runs finetune-noisy-translation-t5-base-bahasa-cased\n",
    "!cd finetune-noisy-translation-t5-base-bahasa-cased && git add . && git commit -m 'add tensorboard' && git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "bleu = BLEU()\n",
    "chrf = CHRF(word_order = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6854"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "import json\n",
    "\n",
    "with open('test-noisy-shuffled.json') as fopen:\n",
    "    test = fopen.read().split('\\n')\n",
    "    test = [json.loads(t) for t in test if len(t)]\n",
    "    \n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6854/6854 [1:10:04<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "results_en_ms, filtered_right_en_ms = [], []\n",
    "results_ms_en, filtered_right_ms_en = [], []\n",
    "for i in tqdm(range(len(test))):\n",
    "    t = test[i]['translation']\n",
    "    p = t['prefix']\n",
    "    s = t['src']\n",
    "    tgt = t['tgt']\n",
    "    \n",
    "    input_ids = [{'input_ids': tokenizer.encode(f'{p}{s}', return_tensors = 'pt')[0]}]\n",
    "    padded = tokenizer.pad(input_ids, padding = 'longest')\n",
    "    outputs = model.generate(**padded, max_length = 1000)[0]\n",
    "    o = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "    if len(o):\n",
    "        if 'Inggeris ke Melayu' in p:\n",
    "            results_en_ms.append(o)\n",
    "            filtered_right_en_ms.append(tgt)\n",
    "        else:\n",
    "            results_ms_en.append(o)\n",
    "            filtered_right_ms_en.append(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2937, 3917)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_en_ms), len(results_ms_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 42.16321973536871,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '73.4/50.1/35.7/25.8 (BP = 0.982 ratio = 0.982 hyp_len = 63335 ref_len = 64473)',\n",
       "  'bp': 0.9821925128801015,\n",
       "  'counts': [46490, 30266, 20534, 14086],\n",
       "  'totals': [63335, 60398, 57461, 54524],\n",
       "  'sys_len': 63335,\n",
       "  'ref_len': 64473,\n",
       "  'precisions': [73.40333149127655,\n",
       "   50.11093082552402,\n",
       "   35.7355423678669,\n",
       "   25.834494901327854],\n",
       "  'prec_str': '73.4/50.1/35.7/25.8',\n",
       "  'ratio': 0.9823492004404945},\n",
       " chrF2++ = 66.51)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = [filtered_right_en_ms]\n",
    "sys = results_en_ms\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 43.432723192596406,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '71.8/49.8/36.6/27.2 (BP = 1.000 ratio = 1.000 hyp_len = 92982 ref_len = 92985)',\n",
       "  'bp': 0.999967736211266,\n",
       "  'counts': [66716, 44323, 31152, 22130],\n",
       "  'totals': [92982, 89065, 85148, 81231],\n",
       "  'sys_len': 92982,\n",
       "  'ref_len': 92985,\n",
       "  'precisions': [71.75152179991827,\n",
       "   49.76477853253242,\n",
       "   36.58570958801146,\n",
       "   27.243293816400143],\n",
       "  'prec_str': '71.8/49.8/36.6/27.2',\n",
       "  'ratio': 0.9999677367317309},\n",
       " chrF2++ = 65.52)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = [filtered_right_ms_en]\n",
    "sys = results_ms_en\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
