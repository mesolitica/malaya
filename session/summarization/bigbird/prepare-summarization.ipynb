{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/summarization/dailymail/translated-dailymail-train.json\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/summarization/cnn-news/translated-cnn-train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'mesolitica-tpu.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/t5/prepare/tokenization.py:135: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import random\n",
    "import collections\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import tokenization\n",
    "import json\n",
    "from google.cloud import storage\n",
    "import mp\n",
    "\n",
    "max_seq_length_encoder = 1536\n",
    "max_seq_length_decoder = 768\n",
    "\n",
    "EOS_ID = 1\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file='pegasus.wordpiece', do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_feature(values):\n",
    "    feature = tf.train.Feature(\n",
    "        int64_list=tf.train.Int64List(value=list(values))\n",
    "    )\n",
    "    return feature\n",
    "\n",
    "def write_instance_to_example_file(X, Y, output_file):\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    for i in range(len(X)):\n",
    "        input_ids = X[i]\n",
    "        target_ids = Y[i]\n",
    "        while len(input_ids) < max_seq_length_encoder:\n",
    "            input_ids.append(0)\n",
    "        while len(target_ids) < max_seq_length_decoder:\n",
    "            target_ids.append(0)\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features['input_ids'] = create_int_feature(input_ids)\n",
    "        features['target_ids'] = create_int_feature(target_ids)\n",
    "        tf_example = tf.train.Example(\n",
    "            features=tf.train.Features(feature=features)\n",
    "        )\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    tf.logging.info('Wrote %d total instances', inst_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(x, y):\n",
    "    tokens = tokenizer.tokenize(x)\n",
    "    if len(tokens) > (max_seq_length_encoder - 2):\n",
    "        tokens = tokens[: max_seq_length_encoder - 2]\n",
    "    tokens = tokens\n",
    "\n",
    "    tokens_y = []\n",
    "    for y_ in y:\n",
    "        tokens_y.extend(tokenizer.tokenize(y_))\n",
    "    if len(tokens_y) > (max_seq_length_decoder - 1):\n",
    "        tokens_y = tokens_y[: max_seq_length_decoder - 1]\n",
    "    \n",
    "    tokens_y = tokenizer.convert_tokens_to_ids(tokens_y)\n",
    "    tokens_y = tokens_y + [EOS_ID]\n",
    "    return tokenizer.convert_tokens_to_ids(tokens), tokens_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_file(X, Y, output_file):\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    for i in range(len(X)):\n",
    "        input_ids = X[i]\n",
    "        target_ids = Y[i]\n",
    "        while len(input_ids) < max_seq_length_encoder:\n",
    "            input_ids.append(0)\n",
    "        while len(target_ids) < max_seq_length_decoder:\n",
    "            target_ids.append(0)\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features['input_ids'] = create_int_feature(input_ids)\n",
    "        features['target_ids'] = create_int_feature(target_ids)\n",
    "        tf_example = tf.train.Example(\n",
    "            features=tf.train.Features(feature=features)\n",
    "        )\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    tf.logging.info('Wrote %d total instances', len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(rows):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket('mesolitica-tpu-general')\n",
    "    rows, index = rows\n",
    "    \n",
    "    X, Y = [], []\n",
    "    output_file = f'bigbird-summarization-{index}.tfrecord'\n",
    "    for i in tqdm(range(len(rows))):\n",
    "        tokens, tokens_y = get_feature(' '.join(data[i]['ms_article']), [' '.join(data[i]['ms_abstract'])])\n",
    "        X.append(tokens)\n",
    "        Y.append(tokens_y)\n",
    "            \n",
    "    write_instance_to_example_file(X, Y, output_file)\n",
    "    blob = bucket.blob(f'bigbird-summarization-data/{output_file}')\n",
    "    blob.upload_from_filename(output_file)\n",
    "    os.system(f'rm {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translated-cnn-train.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "    \n",
    "with open('translated-cnn-train.json') as fopen:\n",
    "    data.extend(json.load(fopen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 8342/9057 [02:21<00:11, 61.47it/s]\n",
      "100%|██████████| 9057/9057 [02:32<00:00, 59.22it/s]\n",
      "100%|██████████| 9057/9057 [02:35<00:00, 58.37it/s]\n",
      "100%|██████████| 9057/9057 [02:33<00:00, 58.84it/s]\n",
      "100%|██████████| 9057/9057 [02:33<00:00, 59.16it/s]\n",
      " 96%|█████████▌| 8694/9057 [02:28<00:06, 60.31it/s]\n",
      "100%|██████████| 9057/9057 [02:34<00:00, 58.64it/s]\n",
      "100%|██████████| 9057/9057 [02:36<00:00, 57.99it/s]\n",
      " 98%|█████████▊| 8873/9057 [02:32<00:03, 57.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Wrote 9057 total instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 8931/9057 [02:33<00:02, 55.21it/s]\n",
      "100%|██████████| 9057/9057 [02:35<00:00, 58.32it/s]\n",
      "100%|██████████| 9057/9057 [02:33<00:00, 59.10it/s]\n",
      "100%|██████████| 9057/9057 [02:36<00:00, 58.01it/s]\n",
      "100%|██████████| 9057/9057 [02:35<00:00, 58.09it/s]\n",
      " 99%|█████████▉| 8995/9057 [02:33<00:00, 71.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Wrote 9057 total instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9057/9057 [02:33<00:00, 59.13it/s]\n",
      "100%|██████████| 9057/9057 [02:36<00:00, 57.81it/s]\n",
      "100%|██████████| 9057/9057 [02:34<00:00, 58.52it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 53.92it/s]t/s]\n",
      " 99%|█████████▊| 8936/9057 [02:33<00:02, 58.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Wrote 18 total instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9057/9057 [02:36<00:00, 57.80it/s]\n",
      "100%|██████████| 9057/9057 [02:34<00:00, 58.57it/s]\n",
      "100%|██████████| 9057/9057 [02:36<00:00, 57.93it/s]\n",
      "100%|██████████| 9057/9057 [02:35<00:00, 58.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n",
      "INFO:tensorflow:Wrote 9057 total instances\n"
     ]
    }
   ],
   "source": [
    "mp.multiprocessing(data, loop, cores = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.to_int32(t)\n",
    "        example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "def input_fn_builder(\n",
    "    input_files,\n",
    "    max_seq_length_encoder,\n",
    "    max_seq_length_decoder,\n",
    "    max_predictions_per_seq,\n",
    "    is_training,\n",
    "    num_cpu_threads = 4,\n",
    "):\n",
    "    def input_fn(params):\n",
    "        batch_size = params['batch_size']\n",
    "\n",
    "        name_to_features = {\n",
    "            'input_ids': tf.FixedLenFeature([max_seq_length_encoder], tf.int64),\n",
    "            'target_ids': tf.FixedLenFeature(\n",
    "                [max_seq_length_decoder], tf.int64\n",
    "            ),\n",
    "        }\n",
    "        if is_training:\n",
    "            d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size = len(input_files))\n",
    "            cycle_length = min(num_cpu_threads, len(input_files))\n",
    "            d = d.apply(\n",
    "                tf.contrib.data.parallel_interleave(\n",
    "                    tf.data.TFRecordDataset,\n",
    "                    sloppy = is_training,\n",
    "                    cycle_length = cycle_length,\n",
    "                )\n",
    "            )\n",
    "            d = d.shuffle(buffer_size = 100)\n",
    "        else:\n",
    "            d = tf.data.TFRecordDataset(input_files)\n",
    "            d = d.repeat()\n",
    "        d = d.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                lambda record: _decode_record(record, name_to_features),\n",
    "                batch_size = batch_size,\n",
    "                num_parallel_batches = num_cpu_threads,\n",
    "                drop_remainder = True,\n",
    "            )\n",
    "        )\n",
    "        return d\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = tf.io.gfile.glob('gs://mesolitica-tpu-general/bigbird-summarization-data/*.tfrecord')\n",
    "input_fn = input_fn_builder(files, 1536, 768, 0, True)\n",
    "dataset = input_fn({'batch_size': 1})\n",
    "dataset = dataset._make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sess.run(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenization\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file='pegasus.wordpiece', do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nick',\n",
       " 'Wil',\n",
       " '##kins',\n",
       " 'didi',\n",
       " '##agnosis',\n",
       " 'dengan',\n",
       " 'le',\n",
       " '##uke',\n",
       " '##mi',\n",
       " '##a',\n",
       " 'ketika',\n",
       " 'dia',\n",
       " 'berusia',\n",
       " '4',\n",
       " 'tahun',\n",
       " ',',\n",
       " 'dan',\n",
       " 'ketika',\n",
       " 'barah',\n",
       " 'itu',\n",
       " 'terus',\n",
       " 'bangkit',\n",
       " 'kembali',\n",
       " ',',\n",
       " 'tidak',\n",
       " 'tahan',\n",
       " 'terhadap',\n",
       " 'semua',\n",
       " 'rawatan',\n",
       " 'yang',\n",
       " 'berbeza',\n",
       " 'yang',\n",
       " 'doktor',\n",
       " 'cuba',\n",
       " ',',\n",
       " 'bapanya',\n",
       " 'duduk',\n",
       " 'dia',\n",
       " 'untuk',\n",
       " 'bercakap',\n",
       " '.',\n",
       " 'John',\n",
       " 'Wil',\n",
       " '##kins',\n",
       " 'menjelaskan',\n",
       " 'kepada',\n",
       " 'Nick',\n",
       " ',',\n",
       " 'yang',\n",
       " 'ketika',\n",
       " 'itu',\n",
       " 'berusia',\n",
       " '14',\n",
       " 'tahun',\n",
       " ',',\n",
       " 'bahawa',\n",
       " 'doktor',\n",
       " 'telah',\n",
       " 'mencuba',\n",
       " 'kem',\n",
       " '##oterapi',\n",
       " ',',\n",
       " 'radiasi',\n",
       " ',',\n",
       " 'malah',\n",
       " 'pemindahan',\n",
       " 'sum',\n",
       " '##sum',\n",
       " 'tulang',\n",
       " 'dari',\n",
       " 'kakaknya',\n",
       " '.',\n",
       " '\"',\n",
       " 'I',\n",
       " 'explain',\n",
       " '##ed',\n",
       " 'to',\n",
       " 'him',\n",
       " 'that',\n",
       " 'we',\n",
       " \"'\",\n",
       " 're',\n",
       " 'run',\n",
       " '##ning',\n",
       " 'out',\n",
       " 'of',\n",
       " 'options',\n",
       " ',',\n",
       " '\"',\n",
       " 'Wil',\n",
       " '##kins',\n",
       " 'rem',\n",
       " '##ember',\n",
       " '##s',\n",
       " 'tel',\n",
       " '##ling',\n",
       " 'his',\n",
       " 'so',\n",
       " '##n',\n",
       " '.',\n",
       " 'Terdapat',\n",
       " 'satu',\n",
       " 'kemungkinan',\n",
       " 'rawatan',\n",
       " 'yang',\n",
       " 'boleh',\n",
       " 'mereka',\n",
       " 'cuba',\n",
       " ':',\n",
       " 'terapi',\n",
       " 'eksperimen',\n",
       " 'di',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Pen',\n",
       " '##ns',\n",
       " '##yl',\n",
       " '##vani',\n",
       " '##a',\n",
       " '.',\n",
       " 'Beliau',\n",
       " 'bertanya',\n",
       " 'kepada',\n",
       " 'anaknya',\n",
       " 'sama',\n",
       " 'ada',\n",
       " 'beliau',\n",
       " 'memahami',\n",
       " 'apa',\n",
       " 'maksudnya',\n",
       " 'jika',\n",
       " 'rawatan',\n",
       " 'ini',\n",
       " 'tidak',\n",
       " 'berjaya',\n",
       " '.',\n",
       " '\"',\n",
       " 'Dia',\n",
       " 'memahami',\n",
       " 'dia',\n",
       " 'boleh',\n",
       " 'mati',\n",
       " ',',\n",
       " '\"',\n",
       " 'kata',\n",
       " 'Wil',\n",
       " '##kins',\n",
       " '.',\n",
       " '\"',\n",
       " 'Dia',\n",
       " 'sangat',\n",
       " 'tabah',\n",
       " '\"',\n",
       " '.',\n",
       " 'Beberapa',\n",
       " 'bulan',\n",
       " 'kemudian',\n",
       " ',',\n",
       " 'Nick',\n",
       " 'mengembara',\n",
       " 'dari',\n",
       " 'rumahnya',\n",
       " 'di',\n",
       " 'Virginia',\n",
       " 'ke',\n",
       " 'Phil',\n",
       " '##ade',\n",
       " '##l',\n",
       " '##ph',\n",
       " '##ia',\n",
       " 'untuk',\n",
       " 'menjadi',\n",
       " 'sebahagian',\n",
       " 'daripada',\n",
       " 'eksperimen',\n",
       " '.',\n",
       " 'Terapi',\n",
       " 'baru',\n",
       " 'ini',\n",
       " 'jelas',\n",
       " 'berbeza',\n",
       " 'dengan',\n",
       " 'rawatan',\n",
       " 'yang',\n",
       " 'pernah',\n",
       " 'diterima',\n",
       " '##nya',\n",
       " 'sebelum',\n",
       " 'ini',\n",
       " ':',\n",
       " 'Daripada',\n",
       " 'menyerang',\n",
       " 'barah',\n",
       " '##nya',\n",
       " 'dengan',\n",
       " 'racun',\n",
       " 'seperti',\n",
       " 'kem',\n",
       " '##oterapi',\n",
       " 'dan',\n",
       " 'radiasi',\n",
       " ',',\n",
       " 'doktor',\n",
       " 'Phil',\n",
       " '##ade',\n",
       " '##l',\n",
       " '##ph',\n",
       " '##ia',\n",
       " 'mengajar',\n",
       " 'sel',\n",
       " 'imun',\n",
       " 'Nick',\n",
       " 'sendiri',\n",
       " 'untuk',\n",
       " 'menjadi',\n",
       " 'lebih',\n",
       " 'mahir',\n",
       " 'dalam',\n",
       " 'membunuh',\n",
       " 'barah',\n",
       " '.',\n",
       " 'Dua',\n",
       " 'bulan',\n",
       " 'kemudian',\n",
       " ',',\n",
       " 'beliau',\n",
       " 'muncul',\n",
       " 'bebas',\n",
       " 'kanser',\n",
       " '.',\n",
       " 'Sudah',\n",
       " 'enam',\n",
       " 'bulan',\n",
       " 'Nick',\n",
       " ',',\n",
       " 'yang',\n",
       " 'kini',\n",
       " 'berusia',\n",
       " '15',\n",
       " 'tahun',\n",
       " ',',\n",
       " 'menerima',\n",
       " 'terapi',\n",
       " 'sel',\n",
       " 'yang',\n",
       " 'diper',\n",
       " '##iba',\n",
       " '##dikan',\n",
       " ',',\n",
       " 'dan',\n",
       " 'doktor',\n",
       " 'masih',\n",
       " 'tidak',\n",
       " 'dapat',\n",
       " 'menemukan',\n",
       " 'jejak',\n",
       " 'le',\n",
       " '##uke',\n",
       " '##mi',\n",
       " '##a',\n",
       " 'dalam',\n",
       " 'sistem',\n",
       " '##nya',\n",
       " '.',\n",
       " 'Memper',\n",
       " '##cayai',\n",
       " 'int',\n",
       " '##ui',\n",
       " '##si',\n",
       " 'beliau',\n",
       " 'membawa',\n",
       " 'kepada',\n",
       " 'dua',\n",
       " 'diagnosis',\n",
       " 'kanser',\n",
       " '.',\n",
       " 'Twe',\n",
       " '##n',\n",
       " '##ty',\n",
       " '-',\n",
       " 'one',\n",
       " 'other',\n",
       " 'young',\n",
       " 'people',\n",
       " 'recei',\n",
       " '##ved',\n",
       " 'the',\n",
       " 'same',\n",
       " 'treatment',\n",
       " 'at',\n",
       " 'The',\n",
       " 'Children',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Hospital',\n",
       " 'of',\n",
       " 'Phil',\n",
       " '##ade',\n",
       " '##l',\n",
       " '##ph',\n",
       " '##ia',\n",
       " ',',\n",
       " 'and',\n",
       " '18',\n",
       " 'of',\n",
       " 'them',\n",
       " ',',\n",
       " 'like',\n",
       " 'Nick',\n",
       " ',',\n",
       " 'we',\n",
       " '##n',\n",
       " '##t',\n",
       " 'into',\n",
       " 'complete',\n",
       " 'rem',\n",
       " '##ission',\n",
       " '-',\n",
       " 'one',\n",
       " 'of',\n",
       " 'them',\n",
       " 'has',\n",
       " 'been',\n",
       " 'dis',\n",
       " '##ease',\n",
       " '-',\n",
       " 'free',\n",
       " 'for',\n",
       " '20',\n",
       " 'month',\n",
       " '##s',\n",
       " '.',\n",
       " 'The',\n",
       " 'Pen',\n",
       " '##n',\n",
       " 'doc',\n",
       " '##tors',\n",
       " 'rel',\n",
       " '##ease',\n",
       " '##d',\n",
       " 'their',\n",
       " 'findings',\n",
       " 'this',\n",
       " 'week',\n",
       " '##end',\n",
       " 'at',\n",
       " 'the',\n",
       " 'ann',\n",
       " '##ual',\n",
       " 'meet',\n",
       " '##ing',\n",
       " 'of',\n",
       " 'the',\n",
       " 'American',\n",
       " 'Society',\n",
       " 'of',\n",
       " 'Hem',\n",
       " '##ato',\n",
       " '##log',\n",
       " '##y',\n",
       " '.',\n",
       " '\"',\n",
       " 'It',\n",
       " 'give',\n",
       " '##s',\n",
       " 'us',\n",
       " 'ho',\n",
       " '##pe',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cur',\n",
       " '##e',\n",
       " ',',\n",
       " '\"',\n",
       " 'Nick',\n",
       " \"'\",\n",
       " 's',\n",
       " 'father',\n",
       " 'says',\n",
       " '.',\n",
       " '\"',\n",
       " 'Mereka',\n",
       " 'benar',\n",
       " '-',\n",
       " 'benar',\n",
       " 'dekat',\n",
       " '.',\n",
       " 'Saya',\n",
       " 'rasa',\n",
       " 'mereka',\n",
       " 'benar',\n",
       " '-',\n",
       " 'benar',\n",
       " 'menyukai',\n",
       " 'sesuatu',\n",
       " '\"',\n",
       " '.',\n",
       " \"'\",\n",
       " 'Satu',\n",
       " 'alam',\n",
       " 'perubatan',\n",
       " 'yang',\n",
       " 'baru',\n",
       " \"'\",\n",
       " 'Pada',\n",
       " 'persidangan',\n",
       " 'itu',\n",
       " ',',\n",
       " 'dua',\n",
       " 'pusat',\n",
       " 'barah',\n",
       " 'lain',\n",
       " '-',\n",
       " 'Memori',\n",
       " '##al',\n",
       " 'Sl',\n",
       " '##oa',\n",
       " '##n',\n",
       " '-',\n",
       " 'Ket',\n",
       " '##teri',\n",
       " '##ng',\n",
       " 'di',\n",
       " 'New',\n",
       " 'York',\n",
       " 'dan',\n",
       " 'Institut',\n",
       " 'Kanser',\n",
       " 'Nasional',\n",
       " '-',\n",
       " 'akan',\n",
       " 'mengumumkan',\n",
       " 'hasil',\n",
       " 'dengan',\n",
       " 'imun',\n",
       " '##oterapi',\n",
       " 'seperti',\n",
       " 'yang',\n",
       " 'diterima',\n",
       " 'Nick',\n",
       " '.',\n",
       " 'Hasilnya',\n",
       " 'menjanjikan',\n",
       " ',',\n",
       " 'terutama',\n",
       " 'mengingat',\n",
       " 'pasien',\n",
       " 'tidak',\n",
       " 'berhasil',\n",
       " 'dengan',\n",
       " 'praktis',\n",
       " 'setiap',\n",
       " 'terapi',\n",
       " 'lainnya',\n",
       " '.',\n",
       " '\"',\n",
       " 'This',\n",
       " 'is',\n",
       " 'abs',\n",
       " '##ol',\n",
       " '##ute',\n",
       " '##ly',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'more',\n",
       " 'exc',\n",
       " '##iti',\n",
       " '##ng',\n",
       " 'adv',\n",
       " '##ance',\n",
       " '##s',\n",
       " 'I',\n",
       " \"'\",\n",
       " 've',\n",
       " 'seen',\n",
       " 'in',\n",
       " 'can',\n",
       " '##ce',\n",
       " '##r',\n",
       " 'the',\n",
       " '##rap',\n",
       " '##y',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " '20',\n",
       " 'years',\n",
       " ',',\n",
       " '\"',\n",
       " 'said',\n",
       " 'Dr',\n",
       " 'David',\n",
       " 'Port',\n",
       " '##er',\n",
       " ',',\n",
       " 'a',\n",
       " 'hem',\n",
       " '##ato',\n",
       " '##logi',\n",
       " '##st',\n",
       " 'and',\n",
       " 'on',\n",
       " '##co',\n",
       " '##logi',\n",
       " '##st',\n",
       " 'at',\n",
       " 'Pen',\n",
       " '##n',\n",
       " '.',\n",
       " '\"',\n",
       " 'Kami',\n",
       " 'telah',\n",
       " 'memasuki',\n",
       " 'bidang',\n",
       " 'perubatan',\n",
       " 'yang',\n",
       " 'baru',\n",
       " '\"',\n",
       " '.',\n",
       " 'Dalam',\n",
       " 'terapi',\n",
       " ',',\n",
       " 'doktor',\n",
       " 'terlebih',\n",
       " 'dahulu',\n",
       " 'membuang',\n",
       " 'sel',\n",
       " '-',\n",
       " 'T',\n",
       " 'pesakit',\n",
       " ',',\n",
       " 'yang',\n",
       " 'memainkan',\n",
       " 'peranan',\n",
       " 'penting',\n",
       " 'dalam',\n",
       " 'sistem',\n",
       " 'imun',\n",
       " '.',\n",
       " 'Mereka',\n",
       " 'kemudiannya',\n",
       " 'memp',\n",
       " '##rog',\n",
       " '##ram',\n",
       " 'semula',\n",
       " 'sel',\n",
       " '-',\n",
       " 'sel',\n",
       " 'dengan',\n",
       " 'memindahkan',\n",
       " 'gen',\n",
       " '-',\n",
       " 'gen',\n",
       " 'baru',\n",
       " '.',\n",
       " 'Setelah',\n",
       " 'dimasukkan',\n",
       " 'kembali',\n",
       " 'ke',\n",
       " 'dalam',\n",
       " 'badan',\n",
       " ',',\n",
       " 'setiap',\n",
       " 'sel',\n",
       " 'yang',\n",
       " 'diubah',\n",
       " 'suai',\n",
       " 'membiak',\n",
       " 'kepada',\n",
       " '10',\n",
       " ',',\n",
       " '000',\n",
       " 'sel',\n",
       " '.',\n",
       " 'Sel',\n",
       " '-',\n",
       " 'sel',\n",
       " '\"',\n",
       " 'pemburu',\n",
       " '\"',\n",
       " 'ini',\n",
       " 'kemudiannya',\n",
       " 'menjejak',\n",
       " 'dan',\n",
       " 'membunuh',\n",
       " 'barah',\n",
       " 'dalam',\n",
       " 'tubuh',\n",
       " 'pesakit',\n",
       " '.',\n",
       " 'Pada',\n",
       " 'dasarnya',\n",
       " ',',\n",
       " 'para',\n",
       " 'penyelidik',\n",
       " 'berusaha',\n",
       " 'melatih',\n",
       " 'tubuh',\n",
       " 'Nick',\n",
       " 'untuk',\n",
       " 'melawan',\n",
       " 'barah',\n",
       " 'dengan',\n",
       " 'cara',\n",
       " 'yang',\n",
       " 'hampir',\n",
       " 'sama',\n",
       " 'badan',\n",
       " 'kita',\n",
       " 'melawan',\n",
       " 'seles',\n",
       " '##ema',\n",
       " 'biasa',\n",
       " '.',\n",
       " 'Tum',\n",
       " '##or',\n",
       " 'Paint',\n",
       " ':',\n",
       " 'Mengubah',\n",
       " 'cara',\n",
       " 'pakar',\n",
       " 'bedah',\n",
       " 'melawan',\n",
       " 'kanser',\n",
       " '.',\n",
       " 'Selain',\n",
       " 'pasien',\n",
       " 'ped',\n",
       " '##iatri',\n",
       " '##k',\n",
       " ',',\n",
       " 'ilmuwan',\n",
       " 'Pen',\n",
       " '##n',\n",
       " 'mencoba',\n",
       " 'terapi',\n",
       " 'tersebut',\n",
       " 'di',\n",
       " '37',\n",
       " 'orang',\n",
       " 'dewasa',\n",
       " 'dengan',\n",
       " 'le',\n",
       " '##uke',\n",
       " '##mi',\n",
       " '##a',\n",
       " ',',\n",
       " 'dan',\n",
       " '12',\n",
       " 'orang',\n",
       " 'menjalani',\n",
       " 'rem',\n",
       " '##isi',\n",
       " 'lengkap',\n",
       " '.',\n",
       " 'Lapan',\n",
       " 'lagi',\n",
       " 'pesakit',\n",
       " 'pergi',\n",
       " 'ke',\n",
       " 'dalam',\n",
       " 'rem',\n",
       " '##isi',\n",
       " 'separa',\n",
       " 'dan',\n",
       " 'melihat',\n",
       " 'beberapa',\n",
       " 'peningkatan',\n",
       " 'dalam',\n",
       " 'penyakit',\n",
       " 'mereka',\n",
       " '.',\n",
       " 'Rawatan',\n",
       " 'itu',\n",
       " 'membuat',\n",
       " 'pesakit',\n",
       " 'mempunyai',\n",
       " 'gejala',\n",
       " 'flu',\n",
       " '##lik',\n",
       " '##e',\n",
       " 'untuk',\n",
       " 'jangka',\n",
       " 'masa',\n",
       " 'yang',\n",
       " 'singkat',\n",
       " '-',\n",
       " 'Nick',\n",
       " 'jatuh',\n",
       " 'sakit',\n",
       " 'sehingga',\n",
       " 'dia',\n",
       " 'berakhir',\n",
       " 'di',\n",
       " 'unit',\n",
       " 'rawatan',\n",
       " 'intensif',\n",
       " 'selama',\n",
       " 'sehari',\n",
       " '-',\n",
       " 'tetapi',\n",
       " 'pesakit',\n",
       " 'terh',\n",
       " '##indar',\n",
       " 'dari',\n",
       " 'beberapa',\n",
       " 'kesan',\n",
       " 'sampingan',\n",
       " 'yang',\n",
       " 'lebih',\n",
       " 'teruk',\n",
       " 'dan',\n",
       " 'tahan',\n",
       " 'lama',\n",
       " 'dari',\n",
       " 'kem',\n",
       " '##oterapi',\n",
       " 'yang',\n",
       " 'luas',\n",
       " '.',\n",
       " 'Pen',\n",
       " '##n',\n",
       " 'will',\n",
       " 'now',\n",
       " 'work',\n",
       " 'with',\n",
       " 'other',\n",
       " 'medical',\n",
       " 'cent',\n",
       " '##res',\n",
       " 'to',\n",
       " 'test',\n",
       " 'the',\n",
       " 'the',\n",
       " '##rap',\n",
       " '##y',\n",
       " 'in',\n",
       " 'more',\n",
       " 'pati',\n",
       " '##ents',\n",
       " ',',\n",
       " 'and',\n",
       " 'they',\n",
       " 'plan',\n",
       " 'to',\n",
       " 'tr',\n",
       " '##y',\n",
       " 'the',\n",
       " 'the',\n",
       " '##rap',\n",
       " '##y',\n",
       " 'out',\n",
       " 'in',\n",
       " 'other',\n",
       " 'types',\n",
       " 'of',\n",
       " 'bl',\n",
       " '##ood',\n",
       " 'can',\n",
       " '##ce',\n",
       " '##r',\n",
       " '##s',\n",
       " 'and',\n",
       " 'later',\n",
       " 'in',\n",
       " 'solid',\n",
       " 'tum',\n",
       " '##ors',\n",
       " '.',\n",
       " 'Siaran',\n",
       " 'akhbar',\n",
       " 'universiti',\n",
       " 'mengatakan',\n",
       " 'bahawa',\n",
       " 'ia',\n",
       " 'mempunyai',\n",
       " 'hubungan',\n",
       " 'pel',\n",
       " '##esenan',\n",
       " 'dengan',\n",
       " 'syarikat',\n",
       " 'far',\n",
       " '##maseutikal',\n",
       " 'Nov',\n",
       " '##arti',\n",
       " '##s',\n",
       " 'dan',\n",
       " '\"',\n",
       " 'mendapat',\n",
       " 'faedah',\n",
       " 'kewangan',\n",
       " 'yang',\n",
       " 'signifikan',\n",
       " '\"',\n",
       " 'dari',\n",
       " 'perbicaraan',\n",
       " ',',\n",
       " 'dan',\n",
       " 'Port',\n",
       " '##er',\n",
       " 'dan',\n",
       " 'pen',\n",
       " '##em',\n",
       " '##u',\n",
       " 'teknologi',\n",
       " 'lain',\n",
       " '\"',\n",
       " 'telah',\n",
       " 'mendapat',\n",
       " 'keuntungan',\n",
       " 'dari',\n",
       " 'segi',\n",
       " 'kewangan',\n",
       " 'dan',\n",
       " '/',\n",
       " 'atau',\n",
       " 'mungkin',\n",
       " 'mendapat',\n",
       " 'keuntungan',\n",
       " 'dari',\n",
       " 'segi',\n",
       " 'kewangan',\n",
       " 'di',\n",
       " 'masa',\n",
       " 'depan',\n",
       " '\"',\n",
       " '.',\n",
       " 'Mencari',\n",
       " 'sel',\n",
       " 'kan',\n",
       " '##ker',\n",
       " 'satu',\n",
       " 'dalam',\n",
       " 'satu',\n",
       " 'juta',\n",
       " '.',\n",
       " 'Persoalan',\n",
       " 'besar',\n",
       " 'adalah',\n",
       " 'apakah',\n",
       " 'le',\n",
       " '##uke',\n",
       " '##mi',\n",
       " '##a',\n",
       " 'Nick',\n",
       " 'akan',\n",
       " 'kembali',\n",
       " '.',\n",
       " 'Doktor',\n",
       " 'berhati',\n",
       " '-',\n",
       " 'hati',\n",
       " 'optimis',\n",
       " '.',\n",
       " 'Kajian',\n",
       " '-',\n",
       " 'kajian',\n",
       " 'hanya',\n",
       " 'telah',\n",
       " 'berjalan',\n",
       " 'sejak',\n",
       " '2010',\n",
       " ',',\n",
       " 'tetapi',\n",
       " 'setakat',\n",
       " 'ini',\n",
       " 'kadar',\n",
       " 'kam',\n",
       " '##buh',\n",
       " 'adalah',\n",
       " 'agak',\n",
       " 'rendah',\n",
       " ':',\n",
       " 'daripada',\n",
       " '18',\n",
       " 'pesakit',\n",
       " 'ped',\n",
       " '##iatri',\n",
       " '##k',\n",
       " 'lain',\n",
       " 'yang',\n",
       " 'pergi',\n",
       " 'ke',\n",
       " 'dalam',\n",
       " 'rem',\n",
       " '##isi',\n",
       " 'lengkap',\n",
       " ',',\n",
       " 'hanya',\n",
       " 'lima',\n",
       " 'telah',\n",
       " 'kam',\n",
       " '##buh',\n",
       " 'dan',\n",
       " 'daripada',\n",
       " '12',\n",
       " 'orang',\n",
       " 'dewasa',\n",
       " 'yang',\n",
       " 'pergi',\n",
       " 'ke',\n",
       " 'rem',\n",
       " '##isi',\n",
       " 'lengkap',\n",
       " ',',\n",
       " 'hanya',\n",
       " 'satu',\n",
       " 'kam',\n",
       " '##buh',\n",
       " '.',\n",
       " 'Sesetengah',\n",
       " 'pesakit',\n",
       " 'dewasa',\n",
       " 'telah',\n",
       " 'bebas',\n",
       " 'daripada',\n",
       " 'kanser',\n",
       " 'dan',\n",
       " 'tanpa',\n",
       " 'kam',\n",
       " '##buh',\n",
       " 'lebih',\n",
       " 'daripada',\n",
       " 'tiga',\n",
       " 'tahun',\n",
       " 'dan',\n",
       " 'mengira',\n",
       " '.',\n",
       " 'Rela',\n",
       " '##ps',\n",
       " 'selepas',\n",
       " 'terapi',\n",
       " 'sel',\n",
       " 'peribadi',\n",
       " 'ini',\n",
       " 'mungkin',\n",
       " 'lebih',\n",
       " 'menjanjikan',\n",
       " 'daripada',\n",
       " 'kam',\n",
       " '##buh',\n",
       " 'selepas',\n",
       " 'kem',\n",
       " '##oterapi',\n",
       " 'atau',\n",
       " 'pemindahan',\n",
       " 'sum',\n",
       " '##sum',\n",
       " 'tulang',\n",
       " ',',\n",
       " 'Port',\n",
       " '##er',\n",
       " 'menjelaskan',\n",
       " '.',\n",
       " 'Pertama',\n",
       " ',',\n",
       " 'dokter',\n",
       " 'merasa',\n",
       " 'senang',\n",
       " 'menemukan',\n",
       " 'sel',\n",
       " '-',\n",
       " 'sel',\n",
       " 'T',\n",
       " 'yang',\n",
       " 'dire',\n",
       " '##dam',\n",
       " 'ulang',\n",
       " '-',\n",
       " 'yang',\n",
       " 'tahu',\n",
       " 'bagaimana',\n",
       " 'memburu',\n",
       " 'dan',\n",
       " 'menyerang',\n",
       " 'kan',\n",
       " '##ker',\n",
       " '-',\n",
       " 'masih',\n",
       " 'hidup',\n",
       " 'di',\n",
       " 'tubuh',\n",
       " 'pasien',\n",
       " 'setelah',\n",
       " 'lebih',\n",
       " 'dari',\n",
       " 'tiga',\n",
       " 'tahun',\n",
       " '.',\n",
       " 'St',\n",
       " '##ig',\n",
       " '##ma',\n",
       " 'berla',\n",
       " '##ma',\n",
       " '-',\n",
       " 'lama',\n",
       " 'kerana',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(r['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nick',\n",
       " 'Wil',\n",
       " '##kins',\n",
       " 'was',\n",
       " 'out',\n",
       " 'of',\n",
       " 'options',\n",
       " 'for',\n",
       " 'bat',\n",
       " '##t',\n",
       " '##ling',\n",
       " 'le',\n",
       " '##uke',\n",
       " '##mi',\n",
       " '##a',\n",
       " '.',\n",
       " 'Beliau',\n",
       " 'kini',\n",
       " 'bebas',\n",
       " 'daripada',\n",
       " 'kanser',\n",
       " 'selepas',\n",
       " 'menjalani',\n",
       " 'rawatan',\n",
       " 'eksperimen',\n",
       " '.',\n",
       " 'Doktor',\n",
       " 'mengajar',\n",
       " 'sel',\n",
       " 'imun',\n",
       " 'Nick',\n",
       " 'untuk',\n",
       " 'menjadi',\n",
       " 'mahir',\n",
       " 'dalam',\n",
       " 'membunuh',\n",
       " 'barah',\n",
       " '.',\n",
       " 'Pakar',\n",
       " 'berharap',\n",
       " 'rawatan',\n",
       " 'itu',\n",
       " 'akan',\n",
       " 'cepat',\n",
       " 'menjadi',\n",
       " 'lebih',\n",
       " 'meluas',\n",
       " '.',\n",
       " '[CLS]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(r['target_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
